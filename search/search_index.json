{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MultiCons","text":"<p>This python package provides an implementation of the MultiCons (Multiple Consensus) algorithm.</p> <p>MultiCons is a consensus clustering method that uses the frequent closed itemset mining technique to find similarities in the base clustering solutions.</p> <p>The implementation aims to follow the original description of the MultiCons method from the references below.</p>"},{"location":"#installation","title":"Installation","text":"<p>MultiCons is available on the Python Package Index (PyPI). It\u2019s installable using <code>pip</code>:</p> <pre><code>pip install multicons\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>To get started, check out some examples or look up the reference API, please visit our documentation page.</p>"},{"location":"#references","title":"References","text":"<p>Atheer A. \u201cA closed patterns-based approach to the consensus clustering problem\u201d. Other [cs.OH]. Universit\u00e9 C\u00f4te d\u2019Azur, 2016. English. .  Retrieved from tel.archives-ouvertes.fr <p>Atheer A., Pasquier N., Precioso F. \u201cUsing Closed Patterns to Solve the Consensus Clustering Problem\u201d. International Journal of Software Engineering and Knowledge Engineering 2016 26:09n10, 1379-1397</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#020-2023-04-11","title":"0.2.0 - 2023-04-11","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Improve library usage of MultiCons by unpinning dependencies and using ranges instead</li> </ul>"},{"location":"CHANGELOG/#010-2022-05-18","title":"0.1.0 - 2022-05-18","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Implement the MultiCons class</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2021 SergioSim</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API Reference","text":"<pre><code>from multicons import MultiCons\n</code></pre> <pre><code>from multicons import (\n    build_membership_matrix, in_ensemble_similarity, linear_closed_itemsets_miner\n)\n</code></pre> <pre><code>from multicons import consensus_function_10\n</code></pre>"},{"location":"api/#multicons.MultiCons","title":"<code>multicons.MultiCons</code>","text":"<p>         Bases: <code>BaseEstimator</code></p> <p>MultiCons (Multiple Consensus) algorithm.</p> <p>MultiCons is a consensus clustering method that uses the frequent closed itemset mining technique to find similarities in the base clustering solutions.</p> <p>Parameters:</p> Name Type Description Default <code>consensus_function</code> <code>str or function</code> <p>Specifies a consensus function to generate clusters from the available instance sets at each iteration. Currently the following consensus functions are available:</p> <ul> <li><code>consensus_function_10</code>: The simplest approach, used by default.     Removes instance sets with inclusion property and groups together     intersecting instance sets.</li> <li><code>consensus_function_12</code>: Similar to <code>consensus_function_10</code>. Uses a     <code>merging_threshold</code> to decide whether to merge the intersecting instance     sets or to split them (removing the intersection from the     bigger set).</li> <li><code>consensus_function_13</code>: A stricter version of <code>consensus_function_12</code>.     Compares the maximal average intersection ratio with the     <code>merging_threshold</code> to decide whether to merge the intersecting instance     sets or to split them.</li> <li><code>consensus_function_14</code>: A version of <code>consensus_function_13</code> that first     searches the maximal intersection ratio among all possible intersections     prior applying a merge or split decision.</li> <li><code>consensus_function_15</code>: A graph based approach. Builds an adjacency     matrix from the intersection matrix using the <code>decision_threshold</code>.     Merges all connected nodes and then splits overlapping instance sets.</li> </ul> <p>To use another consensus function it is possible to pass a function instead of a string value. The function should accept two arguments - a list of sets and an optional <code>merging_threshold</code>, and should update the list of sets in place. See <code>consensus_function_10</code> for an example.</p> <code>'consensus_function_10'</code> <code>similarity_measure</code> <code>str or function</code> <p>Specifies how to compute the similarity between two clustering solutions. Currently the following similarity measures are available:</p> <ul> <li><code>JaccardSimilarity</code>: Indicates that the pair-wise jaccard similarity     measure should be used. This measure is computed with the formula     <code>yy / (yy + ny + yn)</code>     Where: <code>yy</code> is number of times two points belong to same cluster in both     clusterings and <code>ny</code> and <code>yn</code> are the number of times two points belong     to the same cluster in one clustering but not in the other.</li> <li><code>JaccardIndex</code>: Indicates that the set-wise jaccard similarity     coefficient should be used. This measure is computed with the     formula <code>|X \u2229 Y| / (|X| + |Y| - |X \u2229 Y|)</code>     Where: X and Y are the clustering solutions.</li> </ul> <p>To use another similarity measure it is possible to pass a function instead of a string value. The function should accept two arguments - two numeric numpy arrays (representing the two clustering solutions) and should return a numeric score (indicating how similar the clustering solutions are).</p> <code>'JaccardSimilarity'</code> <code>merging_threshold</code> <code>float</code> <p>Specifies the minimum required ratio (calculated from the intersection between two sets over the size of the smaller set) for which the <code>consensus_function</code> should merge two sets. Only applies to <code>consensus_function_12</code>.</p> <code>0.5</code> <code>optimize_label_names</code> <code>bool</code> <p>Indicates whether the label assignment of the clustering partitions should be optimized to maximize the similarity measure score (using the Hungarian algorithm). By default set to <code>False</code> as the default <code>similarity_measure</code> score (\u201cJaccardSimilarity\u201d) does not depend on which labels are assigned to which cluster.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>consensus_function</code> <code>function</code> <p>The consensus function used to generate clusters from the available instance sets at each iteration.</p> <code>consensus_vectors</code> <code>list of numpy arrays</code> <p>The list of proposed consensus clustering candidates.</p> <code>decision_thresholds</code> <code>list of int</code> <p>The list of decision thresholds values, corresponding to the consensus vectors (in the same order). A decision threshold indicates how many base clustering solutions were required to agree (at least) to form sub-clusters.</p> <code>ensemble_similarity</code> <code>list of float</code> <p>The list of ensemble similarity measures corresponding to the consensus vectors.</p> <code>labels_</code> <code>numpy array</code> <p>The recommended consensus candidate.</p> <code>optimize_label_names</code> <code>bool</code> <p>Indicates whether the label assignment of clustering partitions should be optimized or not.</p> <code>recommended</code> <code>int</code> <p>The index of the recommended consensus vector.</p> <code>similarity_measure</code> <code>function</code> <p>The similarity function used to measure the similarity between two clustering solutions.</p> <code>stability</code> <code>list of int</code> <p>The list of stability values, corresponding to the consensus vectors (in the same order). A stability value indicates how many times the same consensus is generated for different decision thresholds.</p> <code>tree_quality</code> <code>float</code> <p>The tree quality measure (between 0 and 1). Higher is better.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>consensus_function</code> or <code>similarity_measure</code> is not a function and not one of the allowed string values (mentioned above).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>class MultiCons(BaseEstimator):\n\"\"\"MultiCons (Multiple Consensus) algorithm.\n\n    MultiCons is a consensus clustering method that uses the frequent closed itemset\n    mining technique to find similarities in the base clustering solutions.\n\n    Args:\n        consensus_function (str or function): Specifies a\n            consensus function to generate clusters from the available instance sets at\n            each iteration.\n            Currently the following consensus functions are available:\n\n            - `consensus_function_10`: The simplest approach, *used by default*.\n                Removes instance sets with inclusion property and groups together\n                intersecting instance sets.\n            - `consensus_function_12`: Similar to `consensus_function_10`. Uses a\n                `merging_threshold` to decide whether to merge the intersecting instance\n                sets or to split them (removing the intersection from the\n                bigger set).\n            - `consensus_function_13`: A stricter version of `consensus_function_12`.\n                Compares the maximal average intersection ratio with the\n                `merging_threshold` to decide whether to merge the intersecting instance\n                sets or to split them.\n            - `consensus_function_14`: A version of `consensus_function_13` that first\n                searches the maximal intersection ratio among all possible intersections\n                prior applying a merge or split decision.\n            - `consensus_function_15`: A graph based approach. Builds an adjacency\n                matrix from the intersection matrix using the `decision_threshold`.\n                Merges all connected nodes and then splits overlapping instance sets.\n\n            To use another consensus function it is possible to pass a function instead\n            of a string value. The function should accept two arguments - a list of sets\n            and an optional `merging_threshold`, and should update the list of sets in\n            place. See `consensus_function_10` for an example.\n        similarity_measure (str or function): Specifies how to compute the similarity\n            between two clustering solutions.\n            Currently the following similarity measures are available:\n\n            - `JaccardSimilarity`: Indicates that the pair-wise jaccard similarity\n                measure should be used. This measure is computed with the formula\n                `yy / (yy + ny + yn)`\n                Where: `yy` is number of times two points belong to same cluster in both\n                clusterings and `ny` and `yn` are the number of times two points belong\n                to the same cluster in one clustering but not in the other.\n            - `JaccardIndex`: Indicates that the set-wise jaccard similarity\n                coefficient should be used. This measure is computed with the\n                formula `|X \u2229 Y| / (|X| + |Y| - |X \u2229 Y|)`\n                Where: X and Y are the clustering solutions.\n\n            To use another similarity measure it is possible to pass a function instead\n            of a string value. The function should accept two arguments - two numeric\n            numpy arrays (representing the two clustering solutions) and should return a\n            numeric score (indicating how similar the clustering solutions are).\n        merging_threshold (float): Specifies the minimum required ratio (calculated from\n            the intersection between two sets over the size of the smaller set) for\n            which the `consensus_function` should merge two sets. Only applies to\n            `consensus_function_12`.\n        optimize_label_names (bool): Indicates whether the label assignment of\n            the clustering partitions should be optimized to maximize the similarity\n            measure score (using the Hungarian algorithm). By default set to `False` as\n            the default `similarity_measure` score (\"JaccardSimilarity\") does not depend\n            on which labels are assigned to which cluster.\n\n    Attributes:\n        consensus_function (function): The consensus function used to generate clusters\n            from the available instance sets at each iteration.\n        consensus_vectors (list of numpy arrays): The list of proposed consensus\n            clustering candidates.\n        decision_thresholds (list of int): The list of decision thresholds values,\n            corresponding to the consensus vectors (in the same order). A decision\n            threshold indicates how many base clustering solutions were required to\n            agree (at least) to form sub-clusters.\n        ensemble_similarity (list of float): The list of ensemble similarity measures\n            corresponding to the consensus vectors.\n        labels_ (numpy array): The recommended consensus candidate.\n        optimize_label_names (bool): Indicates whether the label assignment of\n            clustering partitions should be optimized or not.\n        recommended (int): The index of the recommended consensus vector.\n        similarity_measure (function): The similarity function used to measure the\n            similarity between two clustering solutions.\n        stability (list of int): The list of stability values, corresponding to the\n            consensus vectors (in the same order). A stability value indicates how many\n            times the same consensus is generated for different decision thresholds.\n        tree_quality (float): The tree quality measure (between 0 and 1). Higher is\n            better.\n\n    Raises:\n        ValueError: If `consensus_function` or `similarity_measure` is not a function\n            and not one of the allowed string values (mentioned above).\n    \"\"\"\n\n    # pylint: disable=too-many-instance-attributes\n\n    _consensus_functions = {\n        \"consensus_function_10\": consensus_function_10,\n        \"consensus_function_12\": consensus_function_12,\n        \"consensus_function_13\": consensus_function_13,\n        \"consensus_function_14\": consensus_function_14,\n        \"consensus_function_15\": consensus_function_15,\n    }\n    _similarity_measures = {\n        \"JaccardSimilarity\": jaccard_similarity,\n        \"JaccardIndex\": jaccard_index,\n    }\n\n    def __init__(\n        self,\n        consensus_function: Union[\n            Literal[\n                \"consensus_function_10\",\n                \"consensus_function_12\",\n                \"consensus_function_13\",\n                \"consensus_function_14\",\n                \"consensus_function_15\",\n            ],\n            Callable[[list[np.ndarray]], None],\n        ] = \"consensus_function_10\",\n        merging_threshold: float = 0.5,\n        similarity_measure: Union[\n            Literal[\"JaccardSimilarity\", \"JaccardIndex\"],\n            Callable[[np.ndarray, np.ndarray], int],\n        ] = \"JaccardSimilarity\",\n        optimize_label_names: bool = False,\n    ):\n\"\"\"Initializes MultiCons.\"\"\"\n\n        self.consensus_function = self._parse_argument(\n            \"consensus_function\", self._consensus_functions, consensus_function\n        )\n        self.similarity_measure = self._parse_argument(\n            \"similarity_measure\", self._similarity_measures, similarity_measure\n        )\n        self.merging_threshold = merging_threshold\n        self.optimize_label_names = optimize_label_names\n        self.consensus_vectors = None\n        self.decision_thresholds = None\n        self.ensemble_similarity = None\n        self.labels_ = None\n        self.recommended = None\n        self.stability = None\n        self.tree_quality = None\n\n    def fit(self, X, y=None, sample_weight=None):  # pylint: disable=unused-argument\n\"\"\"Computes the MultiCons consensus.\n\n        Args:\n            X (list of numeric numpy arrays or a pandas Dataframe): Either a list of\n                arrays where each array represents one clustering solution\n                (base clusterings), or a Dataframe representing a binary membership\n                matrix.\n            y: Ignored. Not used, present here for API consistency by convention.\n            sample_weight: Ignored. Not used, present here for API consistency by\n                convention.\n\n        Returns:\n            self: Returns the (fitted) instance itself.\n        \"\"\"\n\n        if isinstance(X, pd.DataFrame):\n            membership_matrix = pd.DataFrame(X, dtype=bool)\n            X = build_base_clusterings(X)\n        else:\n            X = np.array(X, dtype=int)\n            # 2 Calculate in-ensemble similarity\n            # similarity = in_ensemble_similarity(X)\n            # 3 Build the cluster membership matrix M\n            membership_matrix = build_membership_matrix(X)\n\n        # 4 Generate FCPs from M for minsupport = 0\n        # 5 Sort the FCPs in ascending order according to the size of the instance sets\n        frequent_closed_itemsets = linear_closed_itemsets_miner(membership_matrix)\n        # 6 MaxDT \u2190 length(BaseClusterings)\n        max_d_t = len(X)\n        # 7 BiClust \u2190 {instance sets of FCPs built from MaxDT base clusters}\n        bi_clust = build_bi_clust(membership_matrix, frequent_closed_itemsets, max_d_t)\n        # 8 Assign a label to each set in BiClust to build the first consensus vector\n        #   and store it in a list of vectors ConsVctrs\n        self.consensus_vectors = [0] * max_d_t\n        self.consensus_vectors[max_d_t - 1] = self._assign_labels(bi_clust, X)\n\n        # 9 Build the remaining consensuses\n        # 10 for DT = (MaxDT\u22121) to 1 do\n        for d_t in range(max_d_t - 1, 0, -1):\n            # 11 BiClust \u2190 BiClust \u222a {instance sets of FCPs built from DT base clusters}\n            bi_clust += build_bi_clust(membership_matrix, frequent_closed_itemsets, d_t)\n            # 12 Call the consensus function (Algo. 10)\n            self.consensus_function(bi_clust, self.merging_threshold)\n            # 13 Assign a label to each set in BiClust to build a consensus vector\n            #    and add it to ConsVctrs\n            self.consensus_vectors[d_t - 1] = self._assign_labels(bi_clust, X)\n        # 14 end\n\n        # 15 Remove similar consensuses\n        # 16 ST \u2190 Vector of \u20181\u2019s of length MaxDT\n        self.decision_thresholds = list(range(1, max_d_t + 1))\n        self.stability = [1] * max_d_t\n        # 17 for i = MaxDT to 2 do\n        i = max_d_t - 1\n        while i &gt; 0:\n            # 18 Vi \u2190 ith consensus in ConsVctrs\n            consensus_i = self.consensus_vectors[i]\n            # 19 for j = (i\u22121) to 1 do\n            j = i - 1\n            while j &gt;= 0:\n                # 20 Vj \u2190 jth consensus in ConsVctrs\n                consensus_j = self.consensus_vectors[j]\n                # 21 if Jaccard(Vi , Vj ) = 1 then\n                if self.similarity_measure(consensus_i, consensus_j) == 1:\n                    # 22 ST [i] \u2190 ST [i] + 1\n                    self.stability[i] += 1\n                    # 23 Remove ST [j]\n                    del self.stability[j]\n                    del self.decision_thresholds[j]\n                    # 24 Remove Vj from ConsVctrs\n                    del self.consensus_vectors[j]\n                    i -= 1\n                j -= 1\n            i -= 1\n            # 25 end\n        # 26 end\n\n        # 27 Find the consensus the most similar to the ensemble\n        # 28 L \u2190 length(ConsVctrs)\n        consensus_count = len(self.consensus_vectors)\n        # 29 TSim \u2190 Vector of \u20180\u2019s of length L\n        t_sim = np.zeros(consensus_count)\n        # 30 for i = 1 to L do\n        for i in range(consensus_count):\n            # 31 Ci \u2190 ith consensus in ConsVctrs\n            consensus_i = self.consensus_vectors[i]\n            # 32 for j = 1 to MaxDT do\n            for j in range(max_d_t):\n                # 33 Cj \u2190 jth clustering in BaseClusterings\n                consensus_j = X[j]\n                # 34 TSim[i] \u2190 TSim[i] + Jaccard(Ci,Cj)\n                t_sim[i] += self.similarity_measure(consensus_i, consensus_j)\n            # 35 end\n            # 36 Sim[i] \u2190 TSim[i] / MaxDT\n            t_sim[i] /= max_d_t\n        # 37 end\n        self.recommended = np.where(t_sim == np.amax(t_sim))[0][0]\n        self.labels_ = self.consensus_vectors[self.recommended]\n\n        self.tree_quality = 1\n        if len(np.unique(self.consensus_vectors[0])) == 1:\n            self.tree_quality -= (self.stability[0] - 1) / max(self.decision_thresholds)\n\n        self.ensemble_similarity = t_sim\n        return self\n\n    def cons_tree(self) -&gt; graphviz.Digraph:\n\"\"\"Returns a ConsTree graph. Requires the `fit` method to be called first.\"\"\"\n\n        graph = graphviz.Digraph()\n        graph.attr(\n            \"graph\", label=f\"ConsTree\\nTree Quality = {self.tree_quality}\", labelloc=\"t\"\n        )\n        unique_count = [\n            np.unique(vec, return_counts=True) for vec in self.consensus_vectors\n        ]\n        max_size = len(self.consensus_vectors[0])\n\n        previous = []\n        for i, nodes_count in enumerate(unique_count):\n            attributes = {\n                \"fillcolor\": \"slategray2\",\n                \"shape\": \"ellipse\",\n                \"style\": \"filled\",\n            }\n            if i == self.recommended:\n                attributes.update({\"fillcolor\": \"darkseagreen\", \"shape\": \"box\"})\n            for j in range(len(nodes_count[0])):\n                node_id = f\"{i}{nodes_count[0][j]}\"\n                attributes[\"width\"] = str(int(9 * nodes_count[1][j] / max_size))\n                graph.attr(\"node\", **attributes)\n                graph.node(node_id, str(nodes_count[1][j]))\n                if i == 0:\n                    continue\n                for node in np.unique(\n                    previous[self.consensus_vectors[i] == nodes_count[0][j]]\n                ):\n                    graph.edge(f\"{i - 1}{node}\", node_id)\n\n            previous = self.consensus_vectors[i]\n            with graph.subgraph(name=\"cluster\") as sub_graph:\n                sub_graph.attr(\"graph\", label=\"Legend\")\n                sub_graph.attr(\"node\", shape=\"box\", width=\"\")\n                values = [\n                    f\"DT={self.decision_thresholds[i]}\",\n                    f\"ST={self.stability[i]}\",\n                    f\"Similarity={round(self.ensemble_similarity[i], 2)}\",\n                ]\n                sub_graph.node(f\"legend_{i}\", \" \".join(values))\n                if i &gt; 0:\n                    sub_graph.edge(f\"legend_{i-1}\", f\"legend_{i}\")\n        return graph\n\n    def _assign_labels(self, bi_clust: list[set], base_clusterings: np.ndarray):\n\"\"\"Returns a consensus vector with labels for each instance set in bi_clust.\"\"\"\n\n        result = np.zeros(len(base_clusterings[0]), dtype=int)\n        if not self.optimize_label_names:\n            for i, itemset in enumerate(bi_clust):\n                result[list(itemset)] = i\n            return result\n        unique_labels = np.unique(base_clusterings.flatten()).tolist()\n        max_label = max(unique_labels)\n        for i in range(len(bi_clust) - len(unique_labels)):\n            unique_labels.append(max_label + i + 1)\n\n        cost_matrix = pd.DataFrame(\n            0.0, index=range(len(bi_clust)), columns=unique_labels\n        )\n        for i, itemset in enumerate(map(list, bi_clust)):\n            for j, label in enumerate(unique_labels):\n                labels = np.ones(len(itemset)) * label\n                score = np.array(\n                    [\n                        self.similarity_measure(clustering[itemset], labels)\n                        for clustering in base_clusterings\n                    ]\n                )\n                cost_matrix.loc[i, j] = len(itemset) * (1 + score).sum()\n\n        col_ind = linear_sum_assignment(\n            cost_matrix.apply(lambda x: x.max() - x, axis=1)\n        )[1]\n\n        for i, itemset in enumerate(bi_clust):\n            result[list(itemset)] = col_ind[i]\n        return result\n\n    @staticmethod\n    def _parse_argument(name, arguments, argument) -&gt; Callable:\n\"\"\"Returns the function that corresponds to the argument.\"\"\"\n\n        if callable(argument):\n            return argument\n        value = arguments.get(argument, None)\n        if not value:\n            raise ValueError(\n                f\"Invalid value for `{name}` argument. \"\n                f\"Should be one of ({', '.join(arguments.keys())}) or a function. \"\n                f\"But received `{argument}` instead.\"\n            )\n        return value\n</code></pre>"},{"location":"api/#multicons.core.MultiCons.__init__","title":"<code>__init__(consensus_function='consensus_function_10', merging_threshold=0.5, similarity_measure='JaccardSimilarity', optimize_label_names=False)</code>","text":"<p>Initializes MultiCons.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>def __init__(\n    self,\n    consensus_function: Union[\n        Literal[\n            \"consensus_function_10\",\n            \"consensus_function_12\",\n            \"consensus_function_13\",\n            \"consensus_function_14\",\n            \"consensus_function_15\",\n        ],\n        Callable[[list[np.ndarray]], None],\n    ] = \"consensus_function_10\",\n    merging_threshold: float = 0.5,\n    similarity_measure: Union[\n        Literal[\"JaccardSimilarity\", \"JaccardIndex\"],\n        Callable[[np.ndarray, np.ndarray], int],\n    ] = \"JaccardSimilarity\",\n    optimize_label_names: bool = False,\n):\n\"\"\"Initializes MultiCons.\"\"\"\n\n    self.consensus_function = self._parse_argument(\n        \"consensus_function\", self._consensus_functions, consensus_function\n    )\n    self.similarity_measure = self._parse_argument(\n        \"similarity_measure\", self._similarity_measures, similarity_measure\n    )\n    self.merging_threshold = merging_threshold\n    self.optimize_label_names = optimize_label_names\n    self.consensus_vectors = None\n    self.decision_thresholds = None\n    self.ensemble_similarity = None\n    self.labels_ = None\n    self.recommended = None\n    self.stability = None\n    self.tree_quality = None\n</code></pre>"},{"location":"api/#multicons.core.MultiCons.cons_tree","title":"<code>cons_tree()</code>","text":"<p>Returns a ConsTree graph. Requires the <code>fit</code> method to be called first.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>def cons_tree(self) -&gt; graphviz.Digraph:\n\"\"\"Returns a ConsTree graph. Requires the `fit` method to be called first.\"\"\"\n\n    graph = graphviz.Digraph()\n    graph.attr(\n        \"graph\", label=f\"ConsTree\\nTree Quality = {self.tree_quality}\", labelloc=\"t\"\n    )\n    unique_count = [\n        np.unique(vec, return_counts=True) for vec in self.consensus_vectors\n    ]\n    max_size = len(self.consensus_vectors[0])\n\n    previous = []\n    for i, nodes_count in enumerate(unique_count):\n        attributes = {\n            \"fillcolor\": \"slategray2\",\n            \"shape\": \"ellipse\",\n            \"style\": \"filled\",\n        }\n        if i == self.recommended:\n            attributes.update({\"fillcolor\": \"darkseagreen\", \"shape\": \"box\"})\n        for j in range(len(nodes_count[0])):\n            node_id = f\"{i}{nodes_count[0][j]}\"\n            attributes[\"width\"] = str(int(9 * nodes_count[1][j] / max_size))\n            graph.attr(\"node\", **attributes)\n            graph.node(node_id, str(nodes_count[1][j]))\n            if i == 0:\n                continue\n            for node in np.unique(\n                previous[self.consensus_vectors[i] == nodes_count[0][j]]\n            ):\n                graph.edge(f\"{i - 1}{node}\", node_id)\n\n        previous = self.consensus_vectors[i]\n        with graph.subgraph(name=\"cluster\") as sub_graph:\n            sub_graph.attr(\"graph\", label=\"Legend\")\n            sub_graph.attr(\"node\", shape=\"box\", width=\"\")\n            values = [\n                f\"DT={self.decision_thresholds[i]}\",\n                f\"ST={self.stability[i]}\",\n                f\"Similarity={round(self.ensemble_similarity[i], 2)}\",\n            ]\n            sub_graph.node(f\"legend_{i}\", \" \".join(values))\n            if i &gt; 0:\n                sub_graph.edge(f\"legend_{i-1}\", f\"legend_{i}\")\n    return graph\n</code></pre>"},{"location":"api/#multicons.core.MultiCons.fit","title":"<code>fit(X, y=None, sample_weight=None)</code>","text":"<p>Computes the MultiCons consensus.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>list of numeric numpy arrays or a pandas Dataframe</code> <p>Either a list of arrays where each array represents one clustering solution (base clusterings), or a Dataframe representing a binary membership matrix.</p> required <code>y</code> <p>Ignored. Not used, present here for API consistency by convention.</p> <code>None</code> <code>sample_weight</code> <p>Ignored. Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <p>Returns the (fitted) instance itself.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>def fit(self, X, y=None, sample_weight=None):  # pylint: disable=unused-argument\n\"\"\"Computes the MultiCons consensus.\n\n    Args:\n        X (list of numeric numpy arrays or a pandas Dataframe): Either a list of\n            arrays where each array represents one clustering solution\n            (base clusterings), or a Dataframe representing a binary membership\n            matrix.\n        y: Ignored. Not used, present here for API consistency by convention.\n        sample_weight: Ignored. Not used, present here for API consistency by\n            convention.\n\n    Returns:\n        self: Returns the (fitted) instance itself.\n    \"\"\"\n\n    if isinstance(X, pd.DataFrame):\n        membership_matrix = pd.DataFrame(X, dtype=bool)\n        X = build_base_clusterings(X)\n    else:\n        X = np.array(X, dtype=int)\n        # 2 Calculate in-ensemble similarity\n        # similarity = in_ensemble_similarity(X)\n        # 3 Build the cluster membership matrix M\n        membership_matrix = build_membership_matrix(X)\n\n    # 4 Generate FCPs from M for minsupport = 0\n    # 5 Sort the FCPs in ascending order according to the size of the instance sets\n    frequent_closed_itemsets = linear_closed_itemsets_miner(membership_matrix)\n    # 6 MaxDT \u2190 length(BaseClusterings)\n    max_d_t = len(X)\n    # 7 BiClust \u2190 {instance sets of FCPs built from MaxDT base clusters}\n    bi_clust = build_bi_clust(membership_matrix, frequent_closed_itemsets, max_d_t)\n    # 8 Assign a label to each set in BiClust to build the first consensus vector\n    #   and store it in a list of vectors ConsVctrs\n    self.consensus_vectors = [0] * max_d_t\n    self.consensus_vectors[max_d_t - 1] = self._assign_labels(bi_clust, X)\n\n    # 9 Build the remaining consensuses\n    # 10 for DT = (MaxDT\u22121) to 1 do\n    for d_t in range(max_d_t - 1, 0, -1):\n        # 11 BiClust \u2190 BiClust \u222a {instance sets of FCPs built from DT base clusters}\n        bi_clust += build_bi_clust(membership_matrix, frequent_closed_itemsets, d_t)\n        # 12 Call the consensus function (Algo. 10)\n        self.consensus_function(bi_clust, self.merging_threshold)\n        # 13 Assign a label to each set in BiClust to build a consensus vector\n        #    and add it to ConsVctrs\n        self.consensus_vectors[d_t - 1] = self._assign_labels(bi_clust, X)\n    # 14 end\n\n    # 15 Remove similar consensuses\n    # 16 ST \u2190 Vector of \u20181\u2019s of length MaxDT\n    self.decision_thresholds = list(range(1, max_d_t + 1))\n    self.stability = [1] * max_d_t\n    # 17 for i = MaxDT to 2 do\n    i = max_d_t - 1\n    while i &gt; 0:\n        # 18 Vi \u2190 ith consensus in ConsVctrs\n        consensus_i = self.consensus_vectors[i]\n        # 19 for j = (i\u22121) to 1 do\n        j = i - 1\n        while j &gt;= 0:\n            # 20 Vj \u2190 jth consensus in ConsVctrs\n            consensus_j = self.consensus_vectors[j]\n            # 21 if Jaccard(Vi , Vj ) = 1 then\n            if self.similarity_measure(consensus_i, consensus_j) == 1:\n                # 22 ST [i] \u2190 ST [i] + 1\n                self.stability[i] += 1\n                # 23 Remove ST [j]\n                del self.stability[j]\n                del self.decision_thresholds[j]\n                # 24 Remove Vj from ConsVctrs\n                del self.consensus_vectors[j]\n                i -= 1\n            j -= 1\n        i -= 1\n        # 25 end\n    # 26 end\n\n    # 27 Find the consensus the most similar to the ensemble\n    # 28 L \u2190 length(ConsVctrs)\n    consensus_count = len(self.consensus_vectors)\n    # 29 TSim \u2190 Vector of \u20180\u2019s of length L\n    t_sim = np.zeros(consensus_count)\n    # 30 for i = 1 to L do\n    for i in range(consensus_count):\n        # 31 Ci \u2190 ith consensus in ConsVctrs\n        consensus_i = self.consensus_vectors[i]\n        # 32 for j = 1 to MaxDT do\n        for j in range(max_d_t):\n            # 33 Cj \u2190 jth clustering in BaseClusterings\n            consensus_j = X[j]\n            # 34 TSim[i] \u2190 TSim[i] + Jaccard(Ci,Cj)\n            t_sim[i] += self.similarity_measure(consensus_i, consensus_j)\n        # 35 end\n        # 36 Sim[i] \u2190 TSim[i] / MaxDT\n        t_sim[i] /= max_d_t\n    # 37 end\n    self.recommended = np.where(t_sim == np.amax(t_sim))[0][0]\n    self.labels_ = self.consensus_vectors[self.recommended]\n\n    self.tree_quality = 1\n    if len(np.unique(self.consensus_vectors[0])) == 1:\n        self.tree_quality -= (self.stability[0] - 1) / max(self.decision_thresholds)\n\n    self.ensemble_similarity = t_sim\n    return self\n</code></pre>"},{"location":"api/#multicons.utils","title":"<code>multicons.utils</code>","text":"<p>Utility functions</p>"},{"location":"api/#multicons.utils.build_membership_matrix","title":"<code>build_membership_matrix(base_clusterings)</code>","text":"<p>Computes and returns the membership matrix.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/utils.py</code> <pre><code>def build_membership_matrix(base_clusterings: np.ndarray) -&gt; pd.DataFrame:\n\"\"\"Computes and returns the membership matrix.\"\"\"\n\n    if len(base_clusterings) == 0 or not isinstance(base_clusterings[0], np.ndarray):\n        raise IndexError(\"base_clusterings should contain at least one np.ndarray.\")\n\n    res = []\n    for clusters in base_clusterings:\n        res += [clusters == x for x in np.unique(clusters)]\n    return pd.DataFrame(np.transpose(res), dtype=bool)\n</code></pre>"},{"location":"api/#multicons.utils.in_ensemble_similarity","title":"<code>in_ensemble_similarity(base_clusterings)</code>","text":"<p>Returns the average similarity among the base clusters using Jaccard score.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/utils.py</code> <pre><code>def in_ensemble_similarity(base_clusterings: list[np.ndarray]) -&gt; float:\n\"\"\"Returns the average similarity among the base clusters using Jaccard score.\"\"\"\n\n    if not base_clusterings or len(base_clusterings) &lt; 2:\n        raise IndexError(\"base_clusterings should contain at least two np.ndarrays.\")\n\n    count = len(base_clusterings)\n    index = np.arange(count)\n    similarity = pd.DataFrame(0.0, index=index, columns=index)\n    average_similarity = np.zeros(count)\n    for i in range(count - 1):\n        cluster_i = base_clusterings[i]\n        for j in range(i + 1, count):\n            cluster_j = base_clusterings[j]\n            score = jaccard_index(cluster_i, cluster_j)\n            similarity.iloc[i, j] = similarity.iloc[j, i] = score\n        average_similarity[i] = similarity.iloc[i].sum() / (count - 1)\n\n    average_similarity[count - 1] = similarity.iloc[count - 1].sum() / (count - 1)\n    return np.mean(average_similarity)\n</code></pre>"},{"location":"api/#multicons.utils.linear_closed_itemsets_miner","title":"<code>linear_closed_itemsets_miner(membership_matrix)</code>","text":"<p>Returns a list of frequent closed itemsets using the LCM algorithm.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/utils.py</code> <pre><code>def linear_closed_itemsets_miner(membership_matrix: pd.DataFrame):\n\"\"\"Returns a list of frequent closed itemsets using the LCM algorithm.\"\"\"\n\n    transactions = []\n    for i in membership_matrix.index:\n        transactions.append(np.nonzero(membership_matrix.iloc[i].values)[0].tolist())\n    frequent_closed_itemsets = eclat(transactions, target=\"c\", supp=0, algo=\"o\", conf=0)\n    return sorted(map(lambda x: frozenset(x[0]), frequent_closed_itemsets), key=len)\n</code></pre>"},{"location":"api/#multicons.consensus","title":"<code>multicons.consensus</code>","text":"<p>Consensus functions definitions.</p>"},{"location":"api/#multicons.consensus.consensus_function_10","title":"<code>consensus_function_10(bi_clust, merging_threshold=None)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_10(bi_clust: list[set], merging_threshold=None):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    i = 0\n    count = len(bi_clust)\n    while i &lt; count - 1:\n        bi_clust_i = bi_clust[i]\n        j = i + 1\n        while j &lt; count:\n            bi_clust_j = bi_clust[j]\n            intersection_size = len(bi_clust_i.intersection(bi_clust_j))\n            if intersection_size == 0:\n                j += 1\n                continue\n            if intersection_size == len(bi_clust_i):\n                # Bi\u2282Bj\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                break\n            if intersection_size == len(bi_clust_j):\n                # Bj\u2282Bi\n                del bi_clust[j]\n                count -= 1\n                continue\n            bi_clust[j] = bi_clust_i.union(bi_clust_j)\n            del bi_clust[i]\n            count -= 1\n            i -= 1\n            break\n        i += 1\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_12","title":"<code>consensus_function_12(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_12(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    i = 0\n    count = len(bi_clust)\n    while i &lt; count - 1:\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        j = i + 1\n        while j &lt; count:\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            bi_clust_intersection = bi_clust_i.intersection(bi_clust_j)\n            intersection_size = len(bi_clust_intersection)\n            if intersection_size == 0:\n                j += 1\n                continue\n            if intersection_size == bi_clust_i_size:\n                # Bi\u2282Bj\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                break\n            if intersection_size == bi_clust_j_size:\n                # Bj\u2282Bi\n                del bi_clust[j]\n                count -= 1\n                continue\n            if (\n                intersection_size &gt;= bi_clust_i_size * merging_threshold\n                or intersection_size &gt;= bi_clust_j_size * merging_threshold\n            ):\n                # Merge intersecting sets (Bj\u2229Bi / |Bi| &gt; MT or Bj\u2229Bi / |Bj| &gt; MT)\n                bi_clust[j] = bi_clust_i.union(bi_clust_j)\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                break\n            # Split intersecting sets (remove intersection from bigger set)\n            if bi_clust_i_size &lt;= bi_clust_j_size:\n                bi_clust[j] = bi_clust_j - bi_clust_intersection\n                continue\n            bi_clust[i] = bi_clust_i - bi_clust_intersection\n            i -= 1\n            break\n        i += 1\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_13","title":"<code>consensus_function_13(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_13(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    i = 0\n    count = len(bi_clust)\n    merging_threshold *= 2\n    while i &lt; count - 1:\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        j = i + 1\n        best_intersection_ratio = 0\n        best_intersection_ratio_j = 0\n        broken = False\n        while j &lt; count:\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            bi_clust_intersection = bi_clust_i.intersection(bi_clust_j)\n            intersection_size = len(bi_clust_intersection)\n            if intersection_size == 0:\n                j += 1\n                continue\n            if intersection_size == bi_clust_i_size:\n                # Bi\u2282Bj\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                broken = True\n                break\n            if intersection_size == bi_clust_j_size:\n                # Bj\u2282Bi\n                del bi_clust[j]\n                count -= 1\n                continue\n            average_intersection_ratio = (\n                intersection_size\n                * (bi_clust_j_size + bi_clust_i_size)\n                / (bi_clust_j_size * bi_clust_i_size)\n            )\n            if average_intersection_ratio &gt; best_intersection_ratio:\n                best_intersection_ratio = average_intersection_ratio\n                best_intersection_ratio_j = j\n            j += 1\n\n        if not broken and best_intersection_ratio &gt; 0:\n            if best_intersection_ratio &gt;= merging_threshold:\n                # Merge\n                bi_clust[best_intersection_ratio_j] = bi_clust_i.union(\n                    bi_clust[best_intersection_ratio_j]\n                )\n                del bi_clust[i]\n                count -= 1\n                continue\n            # Split\n            if bi_clust_i_size &lt;= bi_clust_j_size:\n                bi_clust[best_intersection_ratio_j] = (\n                    bi_clust[best_intersection_ratio_j] - bi_clust_i\n                )\n                continue\n            bi_clust[i] = bi_clust_i - bi_clust[best_intersection_ratio_j]\n            continue\n        i += 1\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_14","title":"<code>consensus_function_14(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_14(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    while True:\n        _remove_subsets(bi_clust)\n        bi_clust_size = len(bi_clust)\n        if bi_clust_size == 1:\n            return\n        intersection_matrix = pd.DataFrame(\n            columns=range(bi_clust_size), index=range(bi_clust_size), dtype=int\n        )\n\n        for i in range(bi_clust_size - 1):\n            bi_clust_i = bi_clust[i]\n            bi_clust_i_size = len(bi_clust_i)\n            for j in range(i + 1, bi_clust_size):\n                bi_clust_j = bi_clust[j]\n                bi_clust_j_size = len(bi_clust_j)\n                intersection_size = len(bi_clust_i.intersection(bi_clust_j))\n                if intersection_size == 0:\n                    continue\n                intersection_matrix.iloc[i, j] = intersection_size / bi_clust_i_size\n                intersection_matrix.iloc[j, i] = intersection_size / bi_clust_j_size\n\n        if intersection_matrix.isna().values.all():\n            break\n\n        pointer = pd.DataFrame(columns=range(3), index=range(bi_clust_size))\n        for i in range(bi_clust_size):\n            if not intersection_matrix.iloc[i, :].isna().all():\n                pointer.iloc[i, 0] = i\n                pointer.iloc[i, 1] = intersection_matrix.iloc[i, :].argmax()\n                pointer.iloc[i, 2] = intersection_matrix.iloc[i, pointer.iloc[i, 1]]\n\n        pointer.sort_values(2, inplace=True, ascending=False)\n        pointer = pointer[pointer.iloc[:, 2] &gt; 0.0]\n\n        for k in range(pointer.shape[0]):\n            i = pointer.iloc[k, 0]\n            j = pointer.iloc[k, 1]\n            value = intersection_matrix.iloc[i, j]\n            if value is None:\n                continue\n            if value &gt;= merging_threshold:\n                bi_clust[i] = bi_clust[i].union(bi_clust[j])\n                bi_clust[j] = set()\n                intersection_matrix.iloc[i, :] = None\n                intersection_matrix.iloc[:, j] = None\n                continue\n            if len(bi_clust[i]) &lt;= len(bi_clust[j]):\n                bi_clust[j] = bi_clust[j] - bi_clust[i]\n                intersection_matrix.iloc[j, :] = None\n                intersection_matrix.iloc[:, j] = None\n                continue\n            bi_clust[i] = bi_clust[i] - bi_clust[j]\n            intersection_matrix.iloc[i, :] = None\n            intersection_matrix.iloc[:, i] = None\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_15","title":"<code>consensus_function_15(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_15(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    _remove_subsets(bi_clust)\n    bi_clust_size = len(bi_clust)\n    if bi_clust_size == 1:\n        return\n    intersection_matrix = pd.DataFrame(\n        0, columns=range(bi_clust_size), index=range(bi_clust_size), dtype=int\n    )\n    for i in range(bi_clust_size - 1):\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        for j in range(i + 1, bi_clust_size):\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            intersection_size = len(bi_clust_i.intersection(bi_clust_j))\n            if intersection_size == 0:\n                continue\n            intersection_matrix.iloc[i, j] = intersection_size / bi_clust_i_size\n            intersection_matrix.iloc[j, i] = intersection_size / bi_clust_j_size\n\n    cluster_indexes = sp.coo_matrix(intersection_matrix &gt;= merging_threshold).nonzero()\n    for index, i in enumerate(cluster_indexes[0]):\n        j = cluster_indexes[1][index]\n        bi_clust[i] = bi_clust[j] = bi_clust[i].union(bi_clust[j])\n\n    _remove_subsets(bi_clust)\n    bi_clust_size = len(bi_clust)\n    if bi_clust_size == 1:\n        return\n\n    for i in range(bi_clust_size - 1):\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        for j in range(i + 1, bi_clust_size):\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            bi_clust_intersection = bi_clust_i.intersection(bi_clust_j)\n            if len(bi_clust_intersection) == 0:\n                continue\n            if bi_clust_i_size &lt;= bi_clust_j_size:\n                bi_clust[j] = bi_clust_j - bi_clust_intersection\n                continue\n            bi_clust[i] = bi_clust_i - bi_clust_intersection\n\n    _remove_subsets(bi_clust)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"In\u00a0[1]: Copied! <pre>from os import path\n\nimport numpy as np\nimport pandas as pd\nfrom fcmeans import FCM\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import (\n    DBSCAN,\n    AgglomerativeClustering,\n    Birch,\n    KMeans,\n    SpectralClustering,\n)\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn_extra.cluster import KMedoids\n\nfrom multicons import MultiCons\nfrom multicons.utils import jaccard_similarity\n\nnp.set_printoptions(threshold=100)\n</pre> from os import path  import numpy as np import pandas as pd from fcmeans import FCM from matplotlib import pyplot as plt from sklearn.cluster import (     DBSCAN,     AgglomerativeClustering,     Birch,     KMeans,     SpectralClustering, ) from sklearn.mixture import GaussianMixture from sklearn_extra.cluster import KMedoids  from multicons import MultiCons from multicons.utils import jaccard_similarity  np.set_printoptions(threshold=100) In\u00a0[2]: Copied! <pre># Load the data\nfile_prefix = \"\" if path.exists(\"cassini.csv\") else \"docs/\"\nfile_name = f\"{file_prefix}cassini.csv\"\ncassini = pd.read_csv(file_name)\n# Remove the class labels\ncassini_train_data = cassini.drop(['class'], axis=1)\n</pre> # Load the data file_prefix = \"\" if path.exists(\"cassini.csv\") else \"docs/\" file_name = f\"{file_prefix}cassini.csv\" cassini = pd.read_csv(file_name) # Remove the class labels cassini_train_data = cassini.drop(['class'], axis=1) In\u00a0[3]: Copied! <pre># Setup the plot axes\nfig, axes = plt.subplots(\n    nrows=3, ncols=3, figsize=(18, 12), sharex=True, sharey=True\n)\n# Common plot arguments\ncommon_kwargs = {\"x\": \"x\", \"y\": \"y\", \"colorbar\": False, \"colormap\": \"Paired\"}\n# Our collection of base clusterings\nbase_clusterings = []\n\n# Cassini\ncassini.plot.scatter(c=\"class\", title=\"Cassini\", ax=axes[0, 0], **common_kwargs)\n\n# K-means\nbase_clusterings.append(KMeans(n_clusters=3).fit_predict(cassini_train_data))\ncassini_train_data.plot.scatter(\n    title=\"K-means\", ax=axes[0, 1], c=base_clusterings[-1], **common_kwargs\n)\n\n# Average linkage\nbase_clusterings.append(\n    AgglomerativeClustering(n_clusters=3).fit_predict(cassini_train_data)\n)\ncassini_train_data.plot.scatter(\n    title=\"Average linkage\", ax=axes[0, 2], c=base_clusterings[-1], **common_kwargs\n)\n\n# Gaussian model\nbase_clusterings.append(\n    GaussianMixture(n_components=3, random_state=5).fit_predict(cassini_train_data)\n)\ncassini_train_data.plot.scatter(\n    title=\"Gaussian model\", ax=axes[1, 0], c=base_clusterings[-1], **common_kwargs\n)\n\n# C-means\nfcm = FCM(n_clusters=3, max_iter=5, m=5)\nfcm.fit(cassini_train_data.values)\nbase_clusterings.append(fcm.predict(cassini_train_data.values))\ncassini_train_data.plot.scatter(\n    title=\"C-means\", ax=axes[1, 1], c=base_clusterings[-1], **common_kwargs\n)\n\n# PAM\nbase_clusterings.append(KMedoids(n_clusters=3).fit_predict(cassini_train_data))\ncassini_train_data.plot.scatter(\n    title=\"PAM\", ax=axes[1, 2], c=base_clusterings[-1], **common_kwargs\n)\n\n# BIRCH\nbirch = Birch(n_clusters=3, threshold=0.5)\nbase_clusterings.append(birch.fit_predict(np.ascontiguousarray(cassini_train_data)))\ncassini_train_data.plot.scatter(\n    title=\"BIRCH\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs\n)\n\n# Spectral\nbase_clusterings.append(\n    SpectralClustering(n_clusters=3).fit_predict(cassini_train_data)\n)\ncassini_train_data.plot.scatter(\n    title=\"Spectral\", ax=axes[2, 1], c=base_clusterings[-1], **common_kwargs\n)\n\n# DBSCAN\nbase_clusterings.append(DBSCAN(eps=0.2).fit_predict(cassini_train_data))\ncassini_train_data.plot.scatter(\n    title=\"DBSCAN\", ax=axes[2, 2], c=base_clusterings[-1], **common_kwargs\n)\n\nfig.show()\n</pre> # Setup the plot axes fig, axes = plt.subplots(     nrows=3, ncols=3, figsize=(18, 12), sharex=True, sharey=True ) # Common plot arguments common_kwargs = {\"x\": \"x\", \"y\": \"y\", \"colorbar\": False, \"colormap\": \"Paired\"} # Our collection of base clusterings base_clusterings = []  # Cassini cassini.plot.scatter(c=\"class\", title=\"Cassini\", ax=axes[0, 0], **common_kwargs)  # K-means base_clusterings.append(KMeans(n_clusters=3).fit_predict(cassini_train_data)) cassini_train_data.plot.scatter(     title=\"K-means\", ax=axes[0, 1], c=base_clusterings[-1], **common_kwargs )  # Average linkage base_clusterings.append(     AgglomerativeClustering(n_clusters=3).fit_predict(cassini_train_data) ) cassini_train_data.plot.scatter(     title=\"Average linkage\", ax=axes[0, 2], c=base_clusterings[-1], **common_kwargs )  # Gaussian model base_clusterings.append(     GaussianMixture(n_components=3, random_state=5).fit_predict(cassini_train_data) ) cassini_train_data.plot.scatter(     title=\"Gaussian model\", ax=axes[1, 0], c=base_clusterings[-1], **common_kwargs )  # C-means fcm = FCM(n_clusters=3, max_iter=5, m=5) fcm.fit(cassini_train_data.values) base_clusterings.append(fcm.predict(cassini_train_data.values)) cassini_train_data.plot.scatter(     title=\"C-means\", ax=axes[1, 1], c=base_clusterings[-1], **common_kwargs )  # PAM base_clusterings.append(KMedoids(n_clusters=3).fit_predict(cassini_train_data)) cassini_train_data.plot.scatter(     title=\"PAM\", ax=axes[1, 2], c=base_clusterings[-1], **common_kwargs )  # BIRCH birch = Birch(n_clusters=3, threshold=0.5) base_clusterings.append(birch.fit_predict(np.ascontiguousarray(cassini_train_data))) cassini_train_data.plot.scatter(     title=\"BIRCH\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs )  # Spectral base_clusterings.append(     SpectralClustering(n_clusters=3).fit_predict(cassini_train_data) ) cassini_train_data.plot.scatter(     title=\"Spectral\", ax=axes[2, 1], c=base_clusterings[-1], **common_kwargs )  # DBSCAN base_clusterings.append(DBSCAN(eps=0.2).fit_predict(cassini_train_data)) cassini_train_data.plot.scatter(     title=\"DBSCAN\", ax=axes[2, 2], c=base_clusterings[-1], **common_kwargs )  fig.show() <pre>/home/circleci/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> <p>At this point, <code>base_clusterings</code> now contains all clustering candidates in a list (of lists):</p> In\u00a0[4]: Copied! <pre>np.array(base_clusterings)\n</pre> np.array(base_clusterings) Out[4]: <pre>array([[1, 1, 1, ..., 1, 2, 1],\n       [1, 1, 1, ..., 2, 2, 2],\n       [2, 2, 2, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 2, 1],\n       [0, 0, 0, ..., 2, 2, 2]])</pre> <p>Note: This MultiCons implementation requires the clustering labels to be numerical!</p> <p>Now, let's compute the consensus candidates with MultiCons:</p> In\u00a0[5]: Copied! <pre># MultiCons implementation aims to follow scikit-learn conventions.\nconsensus = MultiCons().fit(base_clusterings)\nconsensus\n</pre> # MultiCons implementation aims to follow scikit-learn conventions. consensus = MultiCons().fit(base_clusterings) consensus Out[5]: <pre>MultiCons(consensus_function=&lt;function consensus_function_10 at 0x7f8307508af0&gt;,\n          similarity_measure=&lt;function jaccard_similarity at 0x7f8307558820&gt;)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultiCons<pre>MultiCons(consensus_function=&lt;function consensus_function_10 at 0x7f8307508af0&gt;,\n          similarity_measure=&lt;function jaccard_similarity at 0x7f8307558820&gt;)</pre> In\u00a0[6]: Copied! <pre># The `consensus_vectors` attribute is a python list containing the\n# consensus candidates.\n# We transform it to a numpy array to better visualize it here.\nnp.array(consensus.consensus_vectors)\n</pre> # The `consensus_vectors` attribute is a python list containing the # consensus candidates. # We transform it to a numpy array to better visualize it here. np.array(consensus.consensus_vectors) Out[6]: <pre>array([[ 0,  0,  0, ...,  0,  0,  0],\n       [ 1,  1,  1, ...,  1,  1,  1],\n       [ 2,  2,  2, ...,  0,  0,  0],\n       [21, 11, 21, ..., 15, 16, 15]])</pre> In\u00a0[7]: Copied! <pre># The `decision_thresholds` attribute contains a list of decision thresholds\n# for each consensus vector.\nconsensus.decision_thresholds\n</pre> # The `decision_thresholds` attribute contains a list of decision thresholds # for each consensus vector. consensus.decision_thresholds Out[7]: <pre>[4, 6, 7, 8]</pre> In\u00a0[8]: Copied! <pre># The `recommended` attribute contains the index of the recommended consensus\n# vector\nconsensus.recommended\n</pre> # The `recommended` attribute contains the index of the recommended consensus # vector consensus.recommended Out[8]: <pre>2</pre> In\u00a0[9]: Copied! <pre># The `labels_` attribute contains the recommended consensus vector\nconsensus.labels_\n</pre> # The `labels_` attribute contains the recommended consensus vector consensus.labels_ Out[9]: <pre>array([2, 2, 2, ..., 0, 0, 0])</pre> In\u00a0[10]: Copied! <pre># Plot the recommended consensus clustering solution\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\ncassini_train_data.plot.scatter(\n    title=\"MultiCons\", ax=axes, c=consensus.labels_, **common_kwargs\n)\nfig.show()\n</pre> # Plot the recommended consensus clustering solution fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4)) cassini_train_data.plot.scatter(     title=\"MultiCons\", ax=axes, c=consensus.labels_, **common_kwargs ) fig.show() In\u00a0[11]: Copied! <pre># The `stability` attribute contains a list of stability values\n# for each consensus vector.\nconsensus.stability\n</pre> # The `stability` attribute contains a list of stability values # for each consensus vector. consensus.stability Out[11]: <pre>[4, 2, 1, 1]</pre> In\u00a0[12]: Copied! <pre># The `tree_quality` member contains a measure of the tree quality.\n# The measure ranges between 0 and 1. Higher is better.\nconsensus.tree_quality\n</pre> # The `tree_quality` member contains a measure of the tree quality. # The measure ranges between 0 and 1. Higher is better. consensus.tree_quality Out[12]: <pre>0.625</pre> In\u00a0[13]: Copied! <pre># The `ensemble_similarity` contains a list of ensemble similarity measures\n# for each consensus vector.\n# They are between 0 and 1. Higher is better.\nconsensus.ensemble_similarity\n</pre> # The `ensemble_similarity` contains a list of ensemble similarity measures # for each consensus vector. # They are between 0 and 1. Higher is better. consensus.ensemble_similarity Out[13]: <pre>array([0.37583408, 0.6889704 , 0.80410075, 0.58600763])</pre> <p>Finally, let's visualize the consenus candidates using the ConsTree method:</p> In\u00a0[14]: Copied! <pre>cons_tree = consensus.cons_tree()\ncons_tree\n</pre> cons_tree = consensus.cons_tree() cons_tree Out[14]: %3 ConsTree Tree Quality = 0.625 cluster Legend 00 1000 10 400 00-&gt;10 11 600 00-&gt;11 legend_0 DT=4 ST=4 Similarity=0.38 legend_1 DT=6 ST=2 Similarity=0.69 legend_0-&gt;legend_1 21 400 10-&gt;21 20 200 11-&gt;20 22 400 11-&gt;22 legend_2 DT=7 ST=1 Similarity=0.8 legend_1-&gt;legend_2 31 1 20-&gt;31 32 1 20-&gt;32 37 3 20-&gt;37 38 3 20-&gt;38 39 5 20-&gt;39 313 17 20-&gt;313 315 27 20-&gt;315 316 42 20-&gt;316 317 46 20-&gt;317 318 55 20-&gt;318 33 1 21-&gt;33 35 2 21-&gt;35 310 6 21-&gt;310 312 14 21-&gt;312 314 18 21-&gt;314 319 170 21-&gt;319 320 189 21-&gt;320 30 1 22-&gt;30 34 1 22-&gt;34 36 3 22-&gt;36 311 12 22-&gt;311 321 383 22-&gt;321 legend_3 DT=8 ST=1 Similarity=0.59 legend_2-&gt;legend_3 In\u00a0[15]: Copied! <pre># Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}CassiniConsTree.svg\", cleanup=True)\n</pre> # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}CassiniConsTree.svg\", cleanup=True) Out[15]: <pre>'docs/CassiniConsTree.svg'</pre> <p>View ConsTree graph in full size: CassiniConsTree.svg</p> In\u00a0[16]: Copied! <pre>cov = np.array([[0.3, 0.1], [0.1, 0.3]])\ngaussian_distributions = pd.DataFrame(\n    np.concatenate(\n        (\n            np.concatenate(\n                (\n                    np.random.multivariate_normal([-0.25, -2], cov, 400),\n                    np.random.multivariate_normal([-1.75, -0.5], cov, 400),\n                    np.random.multivariate_normal([-0.5, 2], cov, 400),\n                    np.random.multivariate_normal([1.75, 2], cov, 400),\n                    np.random.multivariate_normal([2, -0.5], cov, 400),\n                ),\n            ),\n            np.concatenate(\n                (\n                    np.repeat([[1]], 400, 0),\n                    np.repeat([[2]], 400, 0),\n                    np.repeat([[3]], 400, 0),\n                    np.repeat([[4]], 400, 0),\n                    np.repeat([[5]], 400, 0),\n                )\n            )\n        ),\n        axis=1\n    ),\n    columns=[\"x\", \"y\", \"class\"],\n)\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\ngaussian_distributions.plot.scatter(\n    title=\"5 Overlapping Gaussians\", ax=axes, c=\"class\", **common_kwargs\n)\nfig.show()\n</pre> cov = np.array([[0.3, 0.1], [0.1, 0.3]]) gaussian_distributions = pd.DataFrame(     np.concatenate(         (             np.concatenate(                 (                     np.random.multivariate_normal([-0.25, -2], cov, 400),                     np.random.multivariate_normal([-1.75, -0.5], cov, 400),                     np.random.multivariate_normal([-0.5, 2], cov, 400),                     np.random.multivariate_normal([1.75, 2], cov, 400),                     np.random.multivariate_normal([2, -0.5], cov, 400),                 ),             ),             np.concatenate(                 (                     np.repeat([[1]], 400, 0),                     np.repeat([[2]], 400, 0),                     np.repeat([[3]], 400, 0),                     np.repeat([[4]], 400, 0),                     np.repeat([[5]], 400, 0),                 )             )         ),         axis=1     ),     columns=[\"x\", \"y\", \"class\"], ) fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4)) gaussian_distributions.plot.scatter(     title=\"5 Overlapping Gaussians\", ax=axes, c=\"class\", **common_kwargs ) fig.show() In\u00a0[17]: Copied! <pre># Remove the class labels\ngaussian_train_data = gaussian_distributions.drop(['class'], axis=1)\n</pre> # Remove the class labels gaussian_train_data = gaussian_distributions.drop(['class'], axis=1) <p>Next, let's compute the base clusterings and visualize their outcome:</p> In\u00a0[18]: Copied! <pre># Setup the plot axes\nfig, axes = plt.subplots(\n    nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True\n)\n# Our collection of base clusterings\nbase_clusterings = []\n\n# K-means (4 clusters)\nbase_clusterings.append(KMeans(n_clusters=4).fit_predict(gaussian_train_data))\ngaussian_train_data.plot.scatter(\n    title=\"K-means (4 clusters)\",\n    ax=axes[0, 0],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# Average linkage (9 clusters)\nbase_clusterings.append(\n    AgglomerativeClustering(n_clusters=9, linkage=\"average\").fit_predict(\n        gaussian_train_data\n    )\n)\ngaussian_train_data.plot.scatter(\n    title=\"Average linkage (9 clusters)\",\n    ax=axes[0, 1],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# Gaussian model (8 clusters)\nbase_clusterings.append(\n    GaussianMixture(n_components=8, random_state=2, reg_covar=0.2).fit_predict(\n        gaussian_train_data\n    )\n)\ngaussian_train_data.plot.scatter(\n    title=\"Gaussian model (8 clusters)\",\n    ax=axes[1, 0],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# C-means (2 clusters)\nfcm = FCM(n_clusters=2, max_iter=5, m=5)\nfcm.fit(gaussian_train_data.values)\nbase_clusterings.append(fcm.predict(gaussian_train_data.values))\ngaussian_train_data.plot.scatter(\n    title=\"C-means (2 clusters)\",\n    ax=axes[1, 1],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# PAM (3 clusters)\nbase_clusterings.append(KMedoids(n_clusters=3).fit_predict(gaussian_train_data))\ngaussian_train_data.plot.scatter(\n    title=\"PAM (3 clusters)\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs\n)\n\n# BIRCH (5 clusters)\nbirch = Birch(n_clusters=6, threshold=0.5)\nbase_clusterings.append(birch.fit_predict(gaussian_train_data))\ngaussian_train_data.plot.scatter(\n    title=\"BIRCH (6 clusters)\",\n    ax=axes[2, 1],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\nfig.show()\n</pre> # Setup the plot axes fig, axes = plt.subplots(     nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True ) # Our collection of base clusterings base_clusterings = []  # K-means (4 clusters) base_clusterings.append(KMeans(n_clusters=4).fit_predict(gaussian_train_data)) gaussian_train_data.plot.scatter(     title=\"K-means (4 clusters)\",     ax=axes[0, 0],     c=base_clusterings[-1],     **common_kwargs )  # Average linkage (9 clusters) base_clusterings.append(     AgglomerativeClustering(n_clusters=9, linkage=\"average\").fit_predict(         gaussian_train_data     ) ) gaussian_train_data.plot.scatter(     title=\"Average linkage (9 clusters)\",     ax=axes[0, 1],     c=base_clusterings[-1],     **common_kwargs )  # Gaussian model (8 clusters) base_clusterings.append(     GaussianMixture(n_components=8, random_state=2, reg_covar=0.2).fit_predict(         gaussian_train_data     ) ) gaussian_train_data.plot.scatter(     title=\"Gaussian model (8 clusters)\",     ax=axes[1, 0],     c=base_clusterings[-1],     **common_kwargs )  # C-means (2 clusters) fcm = FCM(n_clusters=2, max_iter=5, m=5) fcm.fit(gaussian_train_data.values) base_clusterings.append(fcm.predict(gaussian_train_data.values)) gaussian_train_data.plot.scatter(     title=\"C-means (2 clusters)\",     ax=axes[1, 1],     c=base_clusterings[-1],     **common_kwargs )  # PAM (3 clusters) base_clusterings.append(KMedoids(n_clusters=3).fit_predict(gaussian_train_data)) gaussian_train_data.plot.scatter(     title=\"PAM (3 clusters)\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs )  # BIRCH (5 clusters) birch = Birch(n_clusters=6, threshold=0.5) base_clusterings.append(birch.fit_predict(gaussian_train_data)) gaussian_train_data.plot.scatter(     title=\"BIRCH (6 clusters)\",     ax=axes[2, 1],     c=base_clusterings[-1],     **common_kwargs )  fig.show() <pre>/home/circleci/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> <p>Now, let's compute the consensus candidates with MultiCons and visualize their outcome:</p> In\u00a0[19]: Copied! <pre>consensus_1 = MultiCons()\nconsensus_2 = MultiCons(consensus_function=\"consensus_function_12\")\nconsensus_3 = MultiCons(consensus_function=\"consensus_function_13\")\nconsensus_4 = MultiCons(consensus_function=\"consensus_function_14\")\nconsensus_5 = MultiCons(consensus_function=\"consensus_function_15\")\n\nconsensus_1.fit(base_clusterings)\nconsensus_2.fit(base_clusterings)\nconsensus_3.fit(base_clusterings)\nconsensus_4.fit(base_clusterings)\nconsensus_5.fit(base_clusterings)\n\n# Plot the recommended consensus clustering solutions\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 1\",\n    ax=axes[0, 0],\n    c=consensus_1.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 2\",\n    ax=axes[0, 1],\n    c=consensus_2.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 3\",\n    ax=axes[1, 0],\n    c=consensus_3.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 4\",\n    ax=axes[1, 1],\n    c=consensus_4.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 5\",\n    ax=axes[2, 1],\n    c=consensus_5.labels_,\n    **common_kwargs\n)\nfig.show()\n</pre> consensus_1 = MultiCons() consensus_2 = MultiCons(consensus_function=\"consensus_function_12\") consensus_3 = MultiCons(consensus_function=\"consensus_function_13\") consensus_4 = MultiCons(consensus_function=\"consensus_function_14\") consensus_5 = MultiCons(consensus_function=\"consensus_function_15\")  consensus_1.fit(base_clusterings) consensus_2.fit(base_clusterings) consensus_3.fit(base_clusterings) consensus_4.fit(base_clusterings) consensus_5.fit(base_clusterings)  # Plot the recommended consensus clustering solutions fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 1\",     ax=axes[0, 0],     c=consensus_1.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 2\",     ax=axes[0, 1],     c=consensus_2.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 3\",     ax=axes[1, 0],     c=consensus_3.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 4\",     ax=axes[1, 1],     c=consensus_4.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 5\",     ax=axes[2, 1],     c=consensus_5.labels_,     **common_kwargs ) fig.show() <p>Also, let's visualize the ConsTrees:</p> In\u00a0[20]: Copied! <pre>cons_tree = consensus_1.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree1.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_1.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree1.svg\", cleanup=True) cons_tree Out[20]: %3 ConsTree Tree Quality = 0.5 cluster Legend 00 2000 10 1 00-&gt;10 11 1999 00-&gt;11 legend_0 DT=4 ST=4 Similarity=0.28 legend_1 DT=5 ST=1 Similarity=0.28 legend_0-&gt;legend_1 210 1 10-&gt;210 20 1 11-&gt;20 21 1 11-&gt;21 22 1 11-&gt;22 23 1 11-&gt;23 24 1 11-&gt;24 25 1 11-&gt;25 26 1 11-&gt;26 27 1 11-&gt;27 28 1 11-&gt;28 29 1 11-&gt;29 211 1 11-&gt;211 212 2 11-&gt;212 213 2 11-&gt;213 214 2 11-&gt;214 215 2 11-&gt;215 216 2 11-&gt;216 217 2 11-&gt;217 218 2 11-&gt;218 219 2 11-&gt;219 220 3 11-&gt;220 221 5 11-&gt;221 222 7 11-&gt;222 223 8 11-&gt;223 224 8 11-&gt;224 225 8 11-&gt;225 226 9 11-&gt;226 227 9 11-&gt;227 228 15 11-&gt;228 229 19 11-&gt;229 230 27 11-&gt;230 231 69 11-&gt;231 232 100 11-&gt;232 233 104 11-&gt;233 234 219 11-&gt;234 235 285 11-&gt;235 236 342 11-&gt;236 237 358 11-&gt;237 238 377 11-&gt;238 legend_2 DT=6 ST=1 Similarity=0.54 legend_1-&gt;legend_2 <p>View ConsTree graph 1 in full size: GaussianConsTree1.svg</p> In\u00a0[21]: Copied! <pre>cons_tree = consensus_2.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree2.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_2.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree2.svg\", cleanup=True) cons_tree Out[21]: %3 ConsTree Tree Quality = 1 cluster Legend 00 812 11 770 00-&gt;11 12 794 00-&gt;12 01 1188 10 436 01-&gt;10 01-&gt;11 01-&gt;12 legend_0 DT=1 ST=1 Similarity=0.47 legend_1 DT=2 ST=1 Similarity=0.57 legend_0-&gt;legend_1 20 397 10-&gt;20 22 410 10-&gt;22 11-&gt;22 23 399 11-&gt;23 21 397 12-&gt;21 24 397 12-&gt;24 legend_2 DT=3 ST=1 Similarity=0.71 legend_1-&gt;legend_2 32 407 20-&gt;32 30 399 21-&gt;30 22-&gt;32 33 400 22-&gt;33 31 399 23-&gt;31 24-&gt;30 34 395 24-&gt;34 legend_3 DT=4 ST=1 Similarity=0.71 legend_2-&gt;legend_3 41 4 30-&gt;41 42 5 30-&gt;42 48 394 30-&gt;48 49 395 30-&gt;49 31-&gt;42 43 18 31-&gt;43 46 388 31-&gt;46 44 177 32-&gt;44 45 227 32-&gt;45 47 391 32-&gt;47 32-&gt;48 40 1 33-&gt;40 33-&gt;43 33-&gt;47 34-&gt;41 34-&gt;48 legend_4 DT=5 ST=1 Similarity=0.64 legend_3-&gt;legend_4 510 1 40-&gt;510 53 1 41-&gt;53 55 1 41-&gt;55 517 2 41-&gt;517 50 1 42-&gt;50 514 2 42-&gt;514 515 2 42-&gt;515 516 2 43-&gt;516 524 8 43-&gt;524 525 8 43-&gt;525 523 8 44-&gt;523 531 69 44-&gt;531 532 100 44-&gt;532 51 1 45-&gt;51 522 7 45-&gt;522 534 219 45-&gt;534 520 3 46-&gt;520 527 9 46-&gt;527 528 15 46-&gt;528 529 19 46-&gt;529 536 342 46-&gt;536 52 1 47-&gt;52 59 1 47-&gt;59 512 2 47-&gt;512 513 2 47-&gt;513 530 27 47-&gt;530 537 358 47-&gt;537 54 1 48-&gt;54 56 1 48-&gt;56 57 1 48-&gt;57 58 1 48-&gt;58 518 2 48-&gt;518 519 2 48-&gt;519 526 9 48-&gt;526 538 377 48-&gt;538 511 1 49-&gt;511 521 5 49-&gt;521 533 104 49-&gt;533 535 285 49-&gt;535 legend_5 DT=6 ST=1 Similarity=0.54 legend_4-&gt;legend_5 <p>View ConsTree graph 2 in full size: GaussianConsTree2.svg</p> In\u00a0[22]: Copied! <pre>cons_tree = consensus_3.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree3.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_3.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree3.svg\", cleanup=True) cons_tree Out[22]: %3 ConsTree Tree Quality = 1 cluster Legend 00 993 10 818 00-&gt;10 11 398 00-&gt;11 12 784 00-&gt;12 01 1007 01-&gt;10 01-&gt;11 legend_0 DT=1 ST=1 Similarity=0.49 legend_1 DT=2 ST=1 Similarity=0.54 legend_0-&gt;legend_1 21 390 10-&gt;21 23 398 10-&gt;23 24 400 10-&gt;24 20 28 11-&gt;20 11-&gt;21 22 392 12-&gt;22 25 392 12-&gt;25 legend_2 DT=3 ST=1 Similarity=0.7 legend_1-&gt;legend_2 32 13 20-&gt;32 37 385 20-&gt;37 30 2 21-&gt;30 35 19 21-&gt;35 21-&gt;37 33 11 22-&gt;33 311 390 22-&gt;311 23-&gt;30 36 39 23-&gt;36 39 358 23-&gt;39 34 12 24-&gt;34 38 388 24-&gt;38 31 4 25-&gt;31 25-&gt;33 310 379 25-&gt;310 legend_3 DT=4 ST=1 Similarity=0.67 legend_2-&gt;legend_3 40 1 30-&gt;40 414 387 30-&gt;414 41 2 31-&gt;41 43 2 31-&gt;43 45 4 32-&gt;45 47 4 32-&gt;47 416 395 32-&gt;416 42 2 33-&gt;42 415 388 33-&gt;415 44 4 34-&gt;44 411 177 34-&gt;411 49 18 35-&gt;49 35-&gt;414 46 4 36-&gt;46 48 8 36-&gt;48 36-&gt;414 410 34 37-&gt;410 413 351 37-&gt;413 38-&gt;411 412 219 38-&gt;412 39-&gt;414 310-&gt;415 311-&gt;416 legend_4 DT=5 ST=1 Similarity=0.63 legend_3-&gt;legend_4 510 1 40-&gt;510 53 1 41-&gt;53 55 1 41-&gt;55 56 1 42-&gt;56 57 1 42-&gt;57 517 2 43-&gt;517 54 1 44-&gt;54 58 1 44-&gt;58 518 2 44-&gt;518 50 1 45-&gt;50 520 3 45-&gt;520 512 2 46-&gt;512 513 2 46-&gt;513 514 2 47-&gt;514 515 2 47-&gt;515 51 1 48-&gt;51 522 7 48-&gt;522 516 2 49-&gt;516 524 8 49-&gt;524 525 8 49-&gt;525 528 15 410-&gt;528 529 19 410-&gt;529 523 8 411-&gt;523 531 69 411-&gt;531 532 100 411-&gt;532 534 219 412-&gt;534 527 9 413-&gt;527 536 342 413-&gt;536 52 1 414-&gt;52 59 1 414-&gt;59 530 27 414-&gt;530 537 358 414-&gt;537 519 2 415-&gt;519 526 9 415-&gt;526 538 377 415-&gt;538 511 1 416-&gt;511 521 5 416-&gt;521 533 104 416-&gt;533 535 285 416-&gt;535 legend_5 DT=6 ST=1 Similarity=0.54 legend_4-&gt;legend_5 <p>View ConsTree graph 1 in full size: GaussianConsTree3.svg</p> In\u00a0[23]: Copied! <pre>cons_tree = consensus_4.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree4.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_4.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree4.svg\", cleanup=True) cons_tree Out[23]: %3 ConsTree Tree Quality = 1 cluster Legend 00 1007 11 1076 00-&gt;11 01 993 10 924 01-&gt;10 01-&gt;11 legend_0 DT=1 ST=1 Similarity=0.49 legend_1 DT=2 ST=1 Similarity=0.49 legend_0-&gt;legend_1 21 398 10-&gt;21 22 405 10-&gt;22 23 427 10-&gt;23 24 391 10-&gt;24 20 379 11-&gt;20 11-&gt;21 11-&gt;23 legend_2 DT=3 ST=1 Similarity=0.68 legend_1-&gt;legend_2 30 399 20-&gt;30 32 409 20-&gt;32 21-&gt;30 33 398 21-&gt;33 31 398 22-&gt;31 22-&gt;33 23-&gt;32 34 396 23-&gt;34 24-&gt;31 24-&gt;34 legend_3 DT=4 ST=1 Similarity=0.71 legend_2-&gt;legend_3 40 1 30-&gt;40 44 40 30-&gt;44 48 18 30-&gt;48 49 351 30-&gt;49 41 4 31-&gt;41 42 390 31-&gt;42 43 181 31-&gt;43 45 389 32-&gt;45 47 229 32-&gt;47 32-&gt;48 33-&gt;44 46 397 33-&gt;46 34-&gt;43 34-&gt;47 legend_4 DT=5 ST=1 Similarity=0.63 legend_3-&gt;legend_4 510 1 40-&gt;510 53 1 41-&gt;53 55 1 41-&gt;55 517 2 41-&gt;517 56 1 42-&gt;56 57 1 42-&gt;57 519 2 42-&gt;519 526 9 42-&gt;526 538 377 42-&gt;538 54 1 43-&gt;54 58 1 43-&gt;58 518 2 43-&gt;518 523 8 43-&gt;523 531 69 43-&gt;531 532 100 43-&gt;532 50 1 44-&gt;50 514 2 44-&gt;514 520 3 44-&gt;520 528 15 44-&gt;528 529 19 44-&gt;529 52 1 45-&gt;52 59 1 45-&gt;59 512 2 45-&gt;512 530 27 45-&gt;530 537 358 45-&gt;537 511 1 46-&gt;511 515 2 46-&gt;515 521 5 46-&gt;521 533 104 46-&gt;533 535 285 46-&gt;535 51 1 47-&gt;51 513 2 47-&gt;513 522 7 47-&gt;522 534 219 47-&gt;534 516 2 48-&gt;516 524 8 48-&gt;524 525 8 48-&gt;525 527 9 49-&gt;527 536 342 49-&gt;536 legend_5 DT=6 ST=1 Similarity=0.54 legend_4-&gt;legend_5 <p>View ConsTree graph 4 in full size: GaussianConsTree4.svg</p> In\u00a0[24]: Copied! <pre>cons_tree = consensus_5.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree5.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_5.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree5.svg\", cleanup=True) cons_tree Out[24]: %3 ConsTree Tree Quality = 1 cluster Legend 00 1007 10 1216 00-&gt;10 01 993 01-&gt;10 11 784 01-&gt;11 legend_0 DT=1 ST=1 Similarity=0.49 legend_1 DT=2 ST=1 Similarity=0.47 legend_0-&gt;legend_1 20 427 10-&gt;20 21 403 10-&gt;21 22 764 10-&gt;22 23 406 10-&gt;23 11-&gt;21 11-&gt;23 legend_2 DT=3 ST=1 Similarity=0.63 legend_1-&gt;legend_2 30 396 20-&gt;30 31 39 20-&gt;31 32 769 21-&gt;32 34 398 21-&gt;34 22-&gt;32 23-&gt;30 33 398 23-&gt;33 legend_3 DT=4 ST=1 Similarity=0.63 legend_2-&gt;legend_3 41 179 30-&gt;41 42 258 30-&gt;42 31-&gt;42 40 1 32-&gt;40 43 414 32-&gt;43 44 362 32-&gt;44 33-&gt;41 45 396 33-&gt;45 34-&gt;43 46 390 34-&gt;46 legend_4 DT=5 ST=1 Similarity=0.63 legend_3-&gt;legend_4 510 1 40-&gt;510 54 1 41-&gt;54 58 1 41-&gt;58 523 8 41-&gt;523 531 69 41-&gt;531 532 100 41-&gt;532 51 1 42-&gt;51 512 2 42-&gt;512 513 2 42-&gt;513 522 7 42-&gt;522 530 27 42-&gt;530 534 219 42-&gt;534 50 1 43-&gt;50 514 2 43-&gt;514 515 2 43-&gt;515 520 3 43-&gt;520 521 5 43-&gt;521 524 8 43-&gt;524 525 8 43-&gt;525 527 9 43-&gt;527 528 15 43-&gt;528 529 19 43-&gt;529 536 342 43-&gt;536 52 1 44-&gt;52 59 1 44-&gt;59 516 2 44-&gt;516 537 358 44-&gt;537 53 1 45-&gt;53 55 1 45-&gt;55 56 1 45-&gt;56 57 1 45-&gt;57 517 2 45-&gt;517 518 2 45-&gt;518 519 2 45-&gt;519 526 9 45-&gt;526 538 377 45-&gt;538 511 1 46-&gt;511 533 104 46-&gt;533 535 285 46-&gt;535 legend_5 DT=6 ST=1 Similarity=0.54 legend_4-&gt;legend_5 <p>View ConsTree graph 5 in full size: GaussianConsTree5.svg</p> <p>Finally, let's compare the clustering results:</p> In\u00a0[25]: Copied! <pre>true_labels = gaussian_distributions[\"class\"].to_numpy()\n# We compare the results using the pair-wise Jaccard Similarity Measure\npd.DataFrame(\n    [\n        [\"K-means\", jaccard_similarity(base_clusterings[0], true_labels)],\n        [\"Average linkage\", jaccard_similarity(base_clusterings[1], true_labels)],\n        [\"Gaussian model\", jaccard_similarity(base_clusterings[2], true_labels)],\n        [\"C-means\", jaccard_similarity(base_clusterings[3], true_labels)],\n        [\"PAM\", jaccard_similarity(base_clusterings[4], true_labels)],\n        [\"BIRCH\", jaccard_similarity(base_clusterings[5], true_labels)],\n        [\"MultiCons_1\", jaccard_similarity(consensus_1.labels_, true_labels)],\n        [\"MultiCons_2\", jaccard_similarity(consensus_2.labels_, true_labels)],\n        [\"MultiCons_3\", jaccard_similarity(consensus_3.labels_, true_labels)],\n        [\"MultiCons_4\", jaccard_similarity(consensus_4.labels_, true_labels)],\n        [\"MultiCons_5\", jaccard_similarity(consensus_5.labels_, true_labels)],\n    ],\n    columns=[\"Algorithm\", \"Jaccard\"]\n)\n</pre> true_labels = gaussian_distributions[\"class\"].to_numpy() # We compare the results using the pair-wise Jaccard Similarity Measure pd.DataFrame(     [         [\"K-means\", jaccard_similarity(base_clusterings[0], true_labels)],         [\"Average linkage\", jaccard_similarity(base_clusterings[1], true_labels)],         [\"Gaussian model\", jaccard_similarity(base_clusterings[2], true_labels)],         [\"C-means\", jaccard_similarity(base_clusterings[3], true_labels)],         [\"PAM\", jaccard_similarity(base_clusterings[4], true_labels)],         [\"BIRCH\", jaccard_similarity(base_clusterings[5], true_labels)],         [\"MultiCons_1\", jaccard_similarity(consensus_1.labels_, true_labels)],         [\"MultiCons_2\", jaccard_similarity(consensus_2.labels_, true_labels)],         [\"MultiCons_3\", jaccard_similarity(consensus_3.labels_, true_labels)],         [\"MultiCons_4\", jaccard_similarity(consensus_4.labels_, true_labels)],         [\"MultiCons_5\", jaccard_similarity(consensus_5.labels_, true_labels)],     ],     columns=[\"Algorithm\", \"Jaccard\"] ) Out[25]: Algorithm Jaccard 0 K-means 0.653460 1 Average linkage 0.883504 2 Gaussian model 0.890451 3 C-means 0.340181 4 PAM 0.441031 5 BIRCH 0.905361 6 MultiCons_1 0.664322 7 MultiCons_2 0.911564 8 MultiCons_3 0.886644 9 MultiCons_4 0.906335 10 MultiCons_5 0.792688"},{"location":"examples/#examples","title":"Examples\u00b6","text":"<p>In this section we present some usage examples for MultiCons. We replicate the examples presented in the Thesis of Atheer Al-Najdi (A closed patterns-based approach to the consensus clustering problem).</p>"},{"location":"examples/#cassini-dataset","title":"Cassini dataset\u00b6","text":"<p>The dataset consists of 1000 instances, each represents a point in a 2D space, forming a structure of three clusters.</p> <p>As in the Thesis of Atheer Al-Najdi, we will try to cluster the dataset with 8 different clustering algorithms and then compute and visualize the consensus clustering candidates using the MultiCons and ConsTree methods.</p> <p>Let's get a first visual of the dataset and our base clusterings:</p>"},{"location":"examples/#5-overlapping-gaussian-distributions","title":"5 Overlapping Gaussian distributions\u00b6","text":"<p>Replicating the example with a synthetic dataset used in the thesis of Atheer that consist of:</p> <ul> <li>generating 5 overlapping Gaussian distributed points in a 2D features space</li> <li>appying 6 different clustering algorithms with random choices for K values   (in the range [2, 9])</li> <li>comparing the results of 5 different MultiCons consensus solutions (by   altering the consensus functions)</li> </ul> <p>Let's start by generating the dataset:</p>"}]}