{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MultiCons","text":"<p>This python package provides an implementation of the MultiCons (Multiple Consensus) algorithm.</p> <p>MultiCons is a consensus clustering method that uses the frequent closed itemset mining technique to find similarities in the base clustering solutions.</p> <p>The implementation aims to follow the original description of the MultiCons method from the references below.</p>"},{"location":"#installation","title":"Installation","text":"<p>MultiCons is available on the Python Package Index (PyPI). It\u2019s installable using <code>pip</code>:</p> <pre><code>pip install multicons\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>To get started, check out some examples or look up the reference API, please visit our documentation page.</p>"},{"location":"#references","title":"References","text":"<p>Atheer A. \u201cA closed patterns-based approach to the consensus clustering problem\u201d. Other [cs.OH]. Universit\u00e9 C\u00f4te d\u2019Azur, 2016. English. .  Retrieved from tel.archives-ouvertes.fr <p>Atheer A., Pasquier N., Precioso F. \u201cUsing Closed Patterns to Solve the Consensus Clustering Problem\u201d. International Journal of Software Engineering and Knowledge Engineering 2016 26:09n10, 1379-1397</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#020-2023-04-11","title":"0.2.0 - 2023-04-11","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Improve library usage of MultiCons by unpinning dependencies and using ranges instead</li> </ul>"},{"location":"CHANGELOG/#010-2022-05-18","title":"0.1.0 - 2022-05-18","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Implement the MultiCons class</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2021 SergioSim</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API Reference","text":"<pre><code>from multicons import MultiCons\n</code></pre> <pre><code>from multicons import (\n    build_membership_matrix, in_ensemble_similarity, linear_closed_itemsets_miner\n)\n</code></pre> <pre><code>from multicons import consensus_function_10\n</code></pre>"},{"location":"api/#multicons.MultiCons","title":"<code>multicons.MultiCons</code>","text":"<p>         Bases: <code>BaseEstimator</code></p> <p>MultiCons (Multiple Consensus) algorithm.</p> <p>MultiCons is a consensus clustering method that uses the frequent closed itemset mining technique to find similarities in the base clustering solutions.</p> <p>Parameters:</p> Name Type Description Default <code>consensus_function</code> <code>str or function</code> <p>Specifies a consensus function to generate clusters from the available instance sets at each iteration. Currently the following consensus functions are available:</p> <ul> <li><code>consensus_function_10</code>: The simplest approach, used by default.     Removes instance sets with inclusion property and groups together     intersecting instance sets.</li> <li><code>consensus_function_12</code>: Similar to <code>consensus_function_10</code>. Uses a     <code>merging_threshold</code> to decide whether to merge the intersecting instance     sets or to split them (removing the intersection from the     bigger set).</li> <li><code>consensus_function_13</code>: A stricter version of <code>consensus_function_12</code>.     Compares the maximal average intersection ratio with the     <code>merging_threshold</code> to decide whether to merge the intersecting instance     sets or to split them.</li> <li><code>consensus_function_14</code>: A version of <code>consensus_function_13</code> that first     searches the maximal intersection ratio among all possible intersections     prior applying a merge or split decision.</li> <li><code>consensus_function_15</code>: A graph based approach. Builds an adjacency     matrix from the intersection matrix using the <code>decision_threshold</code>.     Merges all connected nodes and then splits overlapping instance sets.</li> </ul> <p>To use another consensus function it is possible to pass a function instead of a string value. The function should accept two arguments - a list of sets and an optional <code>merging_threshold</code>, and should update the list of sets in place. See <code>consensus_function_10</code> for an example.</p> <code>'consensus_function_10'</code> <code>similarity_measure</code> <code>str or function</code> <p>Specifies how to compute the similarity between two clustering solutions. Currently the following similarity measures are available:</p> <ul> <li><code>JaccardSimilarity</code>: Indicates that the pair-wise jaccard similarity     measure should be used. This measure is computed with the formula     <code>yy / (yy + ny + yn)</code>     Where: <code>yy</code> is number of times two points belong to same cluster in both     clusterings and <code>ny</code> and <code>yn</code> are the number of times two points belong     to the same cluster in one clustering but not in the other.</li> <li><code>JaccardIndex</code>: Indicates that the set-wise jaccard similarity     coefficient should be used. This measure is computed with the     formula <code>|X \u2229 Y| / (|X| + |Y| - |X \u2229 Y|)</code>     Where: X and Y are the clustering solutions.</li> </ul> <p>To use another similarity measure it is possible to pass a function instead of a string value. The function should accept two arguments - two numeric numpy arrays (representing the two clustering solutions) and should return a numeric score (indicating how similar the clustering solutions are).</p> <code>'JaccardSimilarity'</code> <code>merging_threshold</code> <code>float</code> <p>Specifies the minimum required ratio (calculated from the intersection between two sets over the size of the smaller set) for which the <code>consensus_function</code> should merge two sets. Only applies to <code>consensus_function_12</code>.</p> <code>0.5</code> <code>optimize_label_names</code> <code>bool</code> <p>Indicates whether the label assignment of the clustering partitions should be optimized to maximize the similarity measure score (using the Hungarian algorithm). By default set to <code>False</code> as the default <code>similarity_measure</code> score (\u201cJaccardSimilarity\u201d) does not depend on which labels are assigned to which cluster.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>consensus_function</code> <code>function</code> <p>The consensus function used to generate clusters from the available instance sets at each iteration.</p> <code>consensus_vectors</code> <code>list of numpy arrays</code> <p>The list of proposed consensus clustering candidates.</p> <code>decision_thresholds</code> <code>list of int</code> <p>The list of decision thresholds values, corresponding to the consensus vectors (in the same order). A decision threshold indicates how many base clustering solutions were required to agree (at least) to form sub-clusters.</p> <code>ensemble_similarity</code> <code>list of float</code> <p>The list of ensemble similarity measures corresponding to the consensus vectors.</p> <code>labels_</code> <code>numpy array</code> <p>The recommended consensus candidate.</p> <code>optimize_label_names</code> <code>bool</code> <p>Indicates whether the label assignment of clustering partitions should be optimized or not.</p> <code>recommended</code> <code>int</code> <p>The index of the recommended consensus vector.</p> <code>similarity_measure</code> <code>function</code> <p>The similarity function used to measure the similarity between two clustering solutions.</p> <code>stability</code> <code>list of int</code> <p>The list of stability values, corresponding to the consensus vectors (in the same order). A stability value indicates how many times the same consensus is generated for different decision thresholds.</p> <code>tree_quality</code> <code>float</code> <p>The tree quality measure (between 0 and 1). Higher is better.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>consensus_function</code> or <code>similarity_measure</code> is not a function and not one of the allowed string values (mentioned above).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>class MultiCons(BaseEstimator):\n\"\"\"MultiCons (Multiple Consensus) algorithm.\n\n    MultiCons is a consensus clustering method that uses the frequent closed itemset\n    mining technique to find similarities in the base clustering solutions.\n\n    Args:\n        consensus_function (str or function): Specifies a\n            consensus function to generate clusters from the available instance sets at\n            each iteration.\n            Currently the following consensus functions are available:\n\n            - `consensus_function_10`: The simplest approach, *used by default*.\n                Removes instance sets with inclusion property and groups together\n                intersecting instance sets.\n            - `consensus_function_12`: Similar to `consensus_function_10`. Uses a\n                `merging_threshold` to decide whether to merge the intersecting instance\n                sets or to split them (removing the intersection from the\n                bigger set).\n            - `consensus_function_13`: A stricter version of `consensus_function_12`.\n                Compares the maximal average intersection ratio with the\n                `merging_threshold` to decide whether to merge the intersecting instance\n                sets or to split them.\n            - `consensus_function_14`: A version of `consensus_function_13` that first\n                searches the maximal intersection ratio among all possible intersections\n                prior applying a merge or split decision.\n            - `consensus_function_15`: A graph based approach. Builds an adjacency\n                matrix from the intersection matrix using the `decision_threshold`.\n                Merges all connected nodes and then splits overlapping instance sets.\n\n            To use another consensus function it is possible to pass a function instead\n            of a string value. The function should accept two arguments - a list of sets\n            and an optional `merging_threshold`, and should update the list of sets in\n            place. See `consensus_function_10` for an example.\n        similarity_measure (str or function): Specifies how to compute the similarity\n            between two clustering solutions.\n            Currently the following similarity measures are available:\n\n            - `JaccardSimilarity`: Indicates that the pair-wise jaccard similarity\n                measure should be used. This measure is computed with the formula\n                `yy / (yy + ny + yn)`\n                Where: `yy` is number of times two points belong to same cluster in both\n                clusterings and `ny` and `yn` are the number of times two points belong\n                to the same cluster in one clustering but not in the other.\n            - `JaccardIndex`: Indicates that the set-wise jaccard similarity\n                coefficient should be used. This measure is computed with the\n                formula `|X \u2229 Y| / (|X| + |Y| - |X \u2229 Y|)`\n                Where: X and Y are the clustering solutions.\n\n            To use another similarity measure it is possible to pass a function instead\n            of a string value. The function should accept two arguments - two numeric\n            numpy arrays (representing the two clustering solutions) and should return a\n            numeric score (indicating how similar the clustering solutions are).\n        merging_threshold (float): Specifies the minimum required ratio (calculated from\n            the intersection between two sets over the size of the smaller set) for\n            which the `consensus_function` should merge two sets. Only applies to\n            `consensus_function_12`.\n        optimize_label_names (bool): Indicates whether the label assignment of\n            the clustering partitions should be optimized to maximize the similarity\n            measure score (using the Hungarian algorithm). By default set to `False` as\n            the default `similarity_measure` score (\"JaccardSimilarity\") does not depend\n            on which labels are assigned to which cluster.\n\n    Attributes:\n        consensus_function (function): The consensus function used to generate clusters\n            from the available instance sets at each iteration.\n        consensus_vectors (list of numpy arrays): The list of proposed consensus\n            clustering candidates.\n        decision_thresholds (list of int): The list of decision thresholds values,\n            corresponding to the consensus vectors (in the same order). A decision\n            threshold indicates how many base clustering solutions were required to\n            agree (at least) to form sub-clusters.\n        ensemble_similarity (list of float): The list of ensemble similarity measures\n            corresponding to the consensus vectors.\n        labels_ (numpy array): The recommended consensus candidate.\n        optimize_label_names (bool): Indicates whether the label assignment of\n            clustering partitions should be optimized or not.\n        recommended (int): The index of the recommended consensus vector.\n        similarity_measure (function): The similarity function used to measure the\n            similarity between two clustering solutions.\n        stability (list of int): The list of stability values, corresponding to the\n            consensus vectors (in the same order). A stability value indicates how many\n            times the same consensus is generated for different decision thresholds.\n        tree_quality (float): The tree quality measure (between 0 and 1). Higher is\n            better.\n\n    Raises:\n        ValueError: If `consensus_function` or `similarity_measure` is not a function\n            and not one of the allowed string values (mentioned above).\n    \"\"\"\n\n    # pylint: disable=too-many-instance-attributes\n\n    _consensus_functions = {\n        \"consensus_function_10\": consensus_function_10,\n        \"consensus_function_12\": consensus_function_12,\n        \"consensus_function_13\": consensus_function_13,\n        \"consensus_function_14\": consensus_function_14,\n        \"consensus_function_15\": consensus_function_15,\n    }\n    _similarity_measures = {\n        \"JaccardSimilarity\": jaccard_similarity,\n        \"JaccardIndex\": jaccard_index,\n    }\n\n    def __init__(\n        self,\n        consensus_function: Union[\n            Literal[\n                \"consensus_function_10\",\n                \"consensus_function_12\",\n                \"consensus_function_13\",\n                \"consensus_function_14\",\n                \"consensus_function_15\",\n            ],\n            Callable[[list[np.ndarray]], None],\n        ] = \"consensus_function_10\",\n        merging_threshold: float = 0.5,\n        similarity_measure: Union[\n            Literal[\"JaccardSimilarity\", \"JaccardIndex\"],\n            Callable[[np.ndarray, np.ndarray], int],\n        ] = \"JaccardSimilarity\",\n        optimize_label_names: bool = False,\n    ):\n\"\"\"Initializes MultiCons.\"\"\"\n\n        self.consensus_function = self._parse_argument(\n            \"consensus_function\", self._consensus_functions, consensus_function\n        )\n        self.similarity_measure = self._parse_argument(\n            \"similarity_measure\", self._similarity_measures, similarity_measure\n        )\n        self.merging_threshold = merging_threshold\n        self.optimize_label_names = optimize_label_names\n        self.consensus_vectors = None\n        self.decision_thresholds = None\n        self.ensemble_similarity = None\n        self.labels_ = None\n        self.recommended = None\n        self.stability = None\n        self.tree_quality = None\n\n    def fit(self, X, y=None, sample_weight=None):  # pylint: disable=unused-argument\n\"\"\"Computes the MultiCons consensus.\n\n        Args:\n            X (list of numeric numpy arrays or a pandas Dataframe): Either a list of\n                arrays where each array represents one clustering solution\n                (base clusterings), or a Dataframe representing a binary membership\n                matrix.\n            y: Ignored. Not used, present here for API consistency by convention.\n            sample_weight: Ignored. Not used, present here for API consistency by\n                convention.\n\n        Returns:\n            self: Returns the (fitted) instance itself.\n        \"\"\"\n\n        if isinstance(X, pd.DataFrame):\n            membership_matrix = pd.DataFrame(X, dtype=bool)\n            X = build_base_clusterings(X)\n        else:\n            X = np.array(X, dtype=int)\n            # 2 Calculate in-ensemble similarity\n            # similarity = in_ensemble_similarity(X)\n            # 3 Build the cluster membership matrix M\n            membership_matrix = build_membership_matrix(X)\n\n        # 4 Generate FCPs from M for minsupport = 0\n        # 5 Sort the FCPs in ascending order according to the size of the instance sets\n        frequent_closed_itemsets = linear_closed_itemsets_miner(membership_matrix)\n        # 6 MaxDT \u2190 length(BaseClusterings)\n        max_d_t = len(X)\n        # 7 BiClust \u2190 {instance sets of FCPs built from MaxDT base clusters}\n        bi_clust = build_bi_clust(membership_matrix, frequent_closed_itemsets, max_d_t)\n        # 8 Assign a label to each set in BiClust to build the first consensus vector\n        #   and store it in a list of vectors ConsVctrs\n        self.consensus_vectors = [0] * max_d_t\n        self.consensus_vectors[max_d_t - 1] = self._assign_labels(bi_clust, X)\n\n        # 9 Build the remaining consensuses\n        # 10 for DT = (MaxDT\u22121) to 1 do\n        for d_t in range(max_d_t - 1, 0, -1):\n            # 11 BiClust \u2190 BiClust \u222a {instance sets of FCPs built from DT base clusters}\n            bi_clust += build_bi_clust(membership_matrix, frequent_closed_itemsets, d_t)\n            # 12 Call the consensus function (Algo. 10)\n            self.consensus_function(bi_clust, self.merging_threshold)\n            # 13 Assign a label to each set in BiClust to build a consensus vector\n            #    and add it to ConsVctrs\n            self.consensus_vectors[d_t - 1] = self._assign_labels(bi_clust, X)\n        # 14 end\n\n        # 15 Remove similar consensuses\n        # 16 ST \u2190 Vector of \u20181\u2019s of length MaxDT\n        self.decision_thresholds = list(range(1, max_d_t + 1))\n        self.stability = [1] * max_d_t\n        # 17 for i = MaxDT to 2 do\n        i = max_d_t - 1\n        while i &gt; 0:\n            # 18 Vi \u2190 ith consensus in ConsVctrs\n            consensus_i = self.consensus_vectors[i]\n            # 19 for j = (i\u22121) to 1 do\n            j = i - 1\n            while j &gt;= 0:\n                # 20 Vj \u2190 jth consensus in ConsVctrs\n                consensus_j = self.consensus_vectors[j]\n                # 21 if Jaccard(Vi , Vj ) = 1 then\n                if self.similarity_measure(consensus_i, consensus_j) == 1:\n                    # 22 ST [i] \u2190 ST [i] + 1\n                    self.stability[i] += 1\n                    # 23 Remove ST [j]\n                    del self.stability[j]\n                    del self.decision_thresholds[j]\n                    # 24 Remove Vj from ConsVctrs\n                    del self.consensus_vectors[j]\n                    i -= 1\n                j -= 1\n            i -= 1\n            # 25 end\n        # 26 end\n\n        # 27 Find the consensus the most similar to the ensemble\n        # 28 L \u2190 length(ConsVctrs)\n        consensus_count = len(self.consensus_vectors)\n        # 29 TSim \u2190 Vector of \u20180\u2019s of length L\n        t_sim = np.zeros(consensus_count)\n        # 30 for i = 1 to L do\n        for i in range(consensus_count):\n            # 31 Ci \u2190 ith consensus in ConsVctrs\n            consensus_i = self.consensus_vectors[i]\n            # 32 for j = 1 to MaxDT do\n            for j in range(max_d_t):\n                # 33 Cj \u2190 jth clustering in BaseClusterings\n                consensus_j = X[j]\n                # 34 TSim[i] \u2190 TSim[i] + Jaccard(Ci,Cj)\n                t_sim[i] += self.similarity_measure(consensus_i, consensus_j)\n            # 35 end\n            # 36 Sim[i] \u2190 TSim[i] / MaxDT\n            t_sim[i] /= max_d_t\n        # 37 end\n        self.recommended = np.where(t_sim == np.amax(t_sim))[0][0]\n        self.labels_ = self.consensus_vectors[self.recommended]\n\n        self.tree_quality = 1\n        if len(np.unique(self.consensus_vectors[0])) == 1:\n            self.tree_quality -= (self.stability[0] - 1) / max(self.decision_thresholds)\n\n        self.ensemble_similarity = t_sim\n        return self\n\n    def cons_tree(self) -&gt; graphviz.Digraph:\n\"\"\"Returns a ConsTree graph. Requires the `fit` method to be called first.\"\"\"\n\n        graph = graphviz.Digraph()\n        graph.attr(\n            \"graph\", label=f\"ConsTree\\nTree Quality = {self.tree_quality}\", labelloc=\"t\"\n        )\n        unique_count = [\n            np.unique(vec, return_counts=True) for vec in self.consensus_vectors\n        ]\n        max_size = len(self.consensus_vectors[0])\n\n        previous = []\n        for i, nodes_count in enumerate(unique_count):\n            attributes = {\n                \"fillcolor\": \"slategray2\",\n                \"shape\": \"ellipse\",\n                \"style\": \"filled\",\n            }\n            if i == self.recommended:\n                attributes.update({\"fillcolor\": \"darkseagreen\", \"shape\": \"box\"})\n            for j in range(len(nodes_count[0])):\n                node_id = f\"{i}{nodes_count[0][j]}\"\n                attributes[\"width\"] = str(int(9 * nodes_count[1][j] / max_size))\n                graph.attr(\"node\", **attributes)\n                graph.node(node_id, str(nodes_count[1][j]))\n                if i == 0:\n                    continue\n                for node in np.unique(\n                    previous[self.consensus_vectors[i] == nodes_count[0][j]]\n                ):\n                    graph.edge(f\"{i - 1}{node}\", node_id)\n\n            previous = self.consensus_vectors[i]\n            with graph.subgraph(name=\"cluster\") as sub_graph:\n                sub_graph.attr(\"graph\", label=\"Legend\")\n                sub_graph.attr(\"node\", shape=\"box\", width=\"\")\n                values = [\n                    f\"DT={self.decision_thresholds[i]}\",\n                    f\"ST={self.stability[i]}\",\n                    f\"Similarity={round(self.ensemble_similarity[i], 2)}\",\n                ]\n                sub_graph.node(f\"legend_{i}\", \" \".join(values))\n                if i &gt; 0:\n                    sub_graph.edge(f\"legend_{i-1}\", f\"legend_{i}\")\n        return graph\n\n    def _assign_labels(self, bi_clust: list[set], base_clusterings: np.ndarray):\n\"\"\"Returns a consensus vector with labels for each instance set in bi_clust.\"\"\"\n\n        result = np.zeros(len(base_clusterings[0]), dtype=int)\n        if not self.optimize_label_names:\n            for i, itemset in enumerate(bi_clust):\n                result[list(itemset)] = i\n            return result\n        unique_labels = np.unique(base_clusterings.flatten()).tolist()\n        max_label = max(unique_labels)\n        for i in range(len(bi_clust) - len(unique_labels)):\n            unique_labels.append(max_label + i + 1)\n\n        cost_matrix = pd.DataFrame(\n            0.0, index=range(len(bi_clust)), columns=unique_labels\n        )\n        for i, itemset in enumerate(map(list, bi_clust)):\n            for j, label in enumerate(unique_labels):\n                labels = np.ones(len(itemset)) * label\n                score = np.array(\n                    [\n                        self.similarity_measure(clustering[itemset], labels)\n                        for clustering in base_clusterings\n                    ]\n                )\n                cost_matrix.loc[i, j] = len(itemset) * (1 + score).sum()\n\n        col_ind = linear_sum_assignment(\n            cost_matrix.apply(lambda x: x.max() - x, axis=1)\n        )[1]\n\n        for i, itemset in enumerate(bi_clust):\n            result[list(itemset)] = col_ind[i]\n        return result\n\n    @staticmethod\n    def _parse_argument(name, arguments, argument) -&gt; Callable:\n\"\"\"Returns the function that corresponds to the argument.\"\"\"\n\n        if callable(argument):\n            return argument\n        value = arguments.get(argument, None)\n        if not value:\n            raise ValueError(\n                f\"Invalid value for `{name}` argument. \"\n                f\"Should be one of ({', '.join(arguments.keys())}) or a function. \"\n                f\"But received `{argument}` instead.\"\n            )\n        return value\n</code></pre>"},{"location":"api/#multicons.core.MultiCons.__init__","title":"<code>__init__(consensus_function='consensus_function_10', merging_threshold=0.5, similarity_measure='JaccardSimilarity', optimize_label_names=False)</code>","text":"<p>Initializes MultiCons.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>def __init__(\n    self,\n    consensus_function: Union[\n        Literal[\n            \"consensus_function_10\",\n            \"consensus_function_12\",\n            \"consensus_function_13\",\n            \"consensus_function_14\",\n            \"consensus_function_15\",\n        ],\n        Callable[[list[np.ndarray]], None],\n    ] = \"consensus_function_10\",\n    merging_threshold: float = 0.5,\n    similarity_measure: Union[\n        Literal[\"JaccardSimilarity\", \"JaccardIndex\"],\n        Callable[[np.ndarray, np.ndarray], int],\n    ] = \"JaccardSimilarity\",\n    optimize_label_names: bool = False,\n):\n\"\"\"Initializes MultiCons.\"\"\"\n\n    self.consensus_function = self._parse_argument(\n        \"consensus_function\", self._consensus_functions, consensus_function\n    )\n    self.similarity_measure = self._parse_argument(\n        \"similarity_measure\", self._similarity_measures, similarity_measure\n    )\n    self.merging_threshold = merging_threshold\n    self.optimize_label_names = optimize_label_names\n    self.consensus_vectors = None\n    self.decision_thresholds = None\n    self.ensemble_similarity = None\n    self.labels_ = None\n    self.recommended = None\n    self.stability = None\n    self.tree_quality = None\n</code></pre>"},{"location":"api/#multicons.core.MultiCons.cons_tree","title":"<code>cons_tree()</code>","text":"<p>Returns a ConsTree graph. Requires the <code>fit</code> method to be called first.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>def cons_tree(self) -&gt; graphviz.Digraph:\n\"\"\"Returns a ConsTree graph. Requires the `fit` method to be called first.\"\"\"\n\n    graph = graphviz.Digraph()\n    graph.attr(\n        \"graph\", label=f\"ConsTree\\nTree Quality = {self.tree_quality}\", labelloc=\"t\"\n    )\n    unique_count = [\n        np.unique(vec, return_counts=True) for vec in self.consensus_vectors\n    ]\n    max_size = len(self.consensus_vectors[0])\n\n    previous = []\n    for i, nodes_count in enumerate(unique_count):\n        attributes = {\n            \"fillcolor\": \"slategray2\",\n            \"shape\": \"ellipse\",\n            \"style\": \"filled\",\n        }\n        if i == self.recommended:\n            attributes.update({\"fillcolor\": \"darkseagreen\", \"shape\": \"box\"})\n        for j in range(len(nodes_count[0])):\n            node_id = f\"{i}{nodes_count[0][j]}\"\n            attributes[\"width\"] = str(int(9 * nodes_count[1][j] / max_size))\n            graph.attr(\"node\", **attributes)\n            graph.node(node_id, str(nodes_count[1][j]))\n            if i == 0:\n                continue\n            for node in np.unique(\n                previous[self.consensus_vectors[i] == nodes_count[0][j]]\n            ):\n                graph.edge(f\"{i - 1}{node}\", node_id)\n\n        previous = self.consensus_vectors[i]\n        with graph.subgraph(name=\"cluster\") as sub_graph:\n            sub_graph.attr(\"graph\", label=\"Legend\")\n            sub_graph.attr(\"node\", shape=\"box\", width=\"\")\n            values = [\n                f\"DT={self.decision_thresholds[i]}\",\n                f\"ST={self.stability[i]}\",\n                f\"Similarity={round(self.ensemble_similarity[i], 2)}\",\n            ]\n            sub_graph.node(f\"legend_{i}\", \" \".join(values))\n            if i &gt; 0:\n                sub_graph.edge(f\"legend_{i-1}\", f\"legend_{i}\")\n    return graph\n</code></pre>"},{"location":"api/#multicons.core.MultiCons.fit","title":"<code>fit(X, y=None, sample_weight=None)</code>","text":"<p>Computes the MultiCons consensus.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>list of numeric numpy arrays or a pandas Dataframe</code> <p>Either a list of arrays where each array represents one clustering solution (base clusterings), or a Dataframe representing a binary membership matrix.</p> required <code>y</code> <p>Ignored. Not used, present here for API consistency by convention.</p> <code>None</code> <code>sample_weight</code> <p>Ignored. Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <p>Returns the (fitted) instance itself.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/core.py</code> <pre><code>def fit(self, X, y=None, sample_weight=None):  # pylint: disable=unused-argument\n\"\"\"Computes the MultiCons consensus.\n\n    Args:\n        X (list of numeric numpy arrays or a pandas Dataframe): Either a list of\n            arrays where each array represents one clustering solution\n            (base clusterings), or a Dataframe representing a binary membership\n            matrix.\n        y: Ignored. Not used, present here for API consistency by convention.\n        sample_weight: Ignored. Not used, present here for API consistency by\n            convention.\n\n    Returns:\n        self: Returns the (fitted) instance itself.\n    \"\"\"\n\n    if isinstance(X, pd.DataFrame):\n        membership_matrix = pd.DataFrame(X, dtype=bool)\n        X = build_base_clusterings(X)\n    else:\n        X = np.array(X, dtype=int)\n        # 2 Calculate in-ensemble similarity\n        # similarity = in_ensemble_similarity(X)\n        # 3 Build the cluster membership matrix M\n        membership_matrix = build_membership_matrix(X)\n\n    # 4 Generate FCPs from M for minsupport = 0\n    # 5 Sort the FCPs in ascending order according to the size of the instance sets\n    frequent_closed_itemsets = linear_closed_itemsets_miner(membership_matrix)\n    # 6 MaxDT \u2190 length(BaseClusterings)\n    max_d_t = len(X)\n    # 7 BiClust \u2190 {instance sets of FCPs built from MaxDT base clusters}\n    bi_clust = build_bi_clust(membership_matrix, frequent_closed_itemsets, max_d_t)\n    # 8 Assign a label to each set in BiClust to build the first consensus vector\n    #   and store it in a list of vectors ConsVctrs\n    self.consensus_vectors = [0] * max_d_t\n    self.consensus_vectors[max_d_t - 1] = self._assign_labels(bi_clust, X)\n\n    # 9 Build the remaining consensuses\n    # 10 for DT = (MaxDT\u22121) to 1 do\n    for d_t in range(max_d_t - 1, 0, -1):\n        # 11 BiClust \u2190 BiClust \u222a {instance sets of FCPs built from DT base clusters}\n        bi_clust += build_bi_clust(membership_matrix, frequent_closed_itemsets, d_t)\n        # 12 Call the consensus function (Algo. 10)\n        self.consensus_function(bi_clust, self.merging_threshold)\n        # 13 Assign a label to each set in BiClust to build a consensus vector\n        #    and add it to ConsVctrs\n        self.consensus_vectors[d_t - 1] = self._assign_labels(bi_clust, X)\n    # 14 end\n\n    # 15 Remove similar consensuses\n    # 16 ST \u2190 Vector of \u20181\u2019s of length MaxDT\n    self.decision_thresholds = list(range(1, max_d_t + 1))\n    self.stability = [1] * max_d_t\n    # 17 for i = MaxDT to 2 do\n    i = max_d_t - 1\n    while i &gt; 0:\n        # 18 Vi \u2190 ith consensus in ConsVctrs\n        consensus_i = self.consensus_vectors[i]\n        # 19 for j = (i\u22121) to 1 do\n        j = i - 1\n        while j &gt;= 0:\n            # 20 Vj \u2190 jth consensus in ConsVctrs\n            consensus_j = self.consensus_vectors[j]\n            # 21 if Jaccard(Vi , Vj ) = 1 then\n            if self.similarity_measure(consensus_i, consensus_j) == 1:\n                # 22 ST [i] \u2190 ST [i] + 1\n                self.stability[i] += 1\n                # 23 Remove ST [j]\n                del self.stability[j]\n                del self.decision_thresholds[j]\n                # 24 Remove Vj from ConsVctrs\n                del self.consensus_vectors[j]\n                i -= 1\n            j -= 1\n        i -= 1\n        # 25 end\n    # 26 end\n\n    # 27 Find the consensus the most similar to the ensemble\n    # 28 L \u2190 length(ConsVctrs)\n    consensus_count = len(self.consensus_vectors)\n    # 29 TSim \u2190 Vector of \u20180\u2019s of length L\n    t_sim = np.zeros(consensus_count)\n    # 30 for i = 1 to L do\n    for i in range(consensus_count):\n        # 31 Ci \u2190 ith consensus in ConsVctrs\n        consensus_i = self.consensus_vectors[i]\n        # 32 for j = 1 to MaxDT do\n        for j in range(max_d_t):\n            # 33 Cj \u2190 jth clustering in BaseClusterings\n            consensus_j = X[j]\n            # 34 TSim[i] \u2190 TSim[i] + Jaccard(Ci,Cj)\n            t_sim[i] += self.similarity_measure(consensus_i, consensus_j)\n        # 35 end\n        # 36 Sim[i] \u2190 TSim[i] / MaxDT\n        t_sim[i] /= max_d_t\n    # 37 end\n    self.recommended = np.where(t_sim == np.amax(t_sim))[0][0]\n    self.labels_ = self.consensus_vectors[self.recommended]\n\n    self.tree_quality = 1\n    if len(np.unique(self.consensus_vectors[0])) == 1:\n        self.tree_quality -= (self.stability[0] - 1) / max(self.decision_thresholds)\n\n    self.ensemble_similarity = t_sim\n    return self\n</code></pre>"},{"location":"api/#multicons.utils","title":"<code>multicons.utils</code>","text":"<p>Utility functions</p>"},{"location":"api/#multicons.utils.build_membership_matrix","title":"<code>build_membership_matrix(base_clusterings)</code>","text":"<p>Computes and returns the membership matrix.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/utils.py</code> <pre><code>def build_membership_matrix(base_clusterings: np.ndarray) -&gt; pd.DataFrame:\n\"\"\"Computes and returns the membership matrix.\"\"\"\n\n    if len(base_clusterings) == 0 or not isinstance(base_clusterings[0], np.ndarray):\n        raise IndexError(\"base_clusterings should contain at least one np.ndarray.\")\n\n    res = []\n    for clusters in base_clusterings:\n        res += [clusters == x for x in np.unique(clusters)]\n    return pd.DataFrame(np.transpose(res), dtype=bool)\n</code></pre>"},{"location":"api/#multicons.utils.in_ensemble_similarity","title":"<code>in_ensemble_similarity(base_clusterings)</code>","text":"<p>Returns the average similarity among the base clusters using Jaccard score.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/utils.py</code> <pre><code>def in_ensemble_similarity(base_clusterings: list[np.ndarray]) -&gt; float:\n\"\"\"Returns the average similarity among the base clusters using Jaccard score.\"\"\"\n\n    if not base_clusterings or len(base_clusterings) &lt; 2:\n        raise IndexError(\"base_clusterings should contain at least two np.ndarrays.\")\n\n    count = len(base_clusterings)\n    index = np.arange(count)\n    similarity = pd.DataFrame(0.0, index=index, columns=index)\n    average_similarity = np.zeros(count)\n    for i in range(count - 1):\n        cluster_i = base_clusterings[i]\n        for j in range(i + 1, count):\n            cluster_j = base_clusterings[j]\n            score = jaccard_index(cluster_i, cluster_j)\n            similarity.iloc[i, j] = similarity.iloc[j, i] = score\n        average_similarity[i] = similarity.iloc[i].sum() / (count - 1)\n\n    average_similarity[count - 1] = similarity.iloc[count - 1].sum() / (count - 1)\n    return np.mean(average_similarity)\n</code></pre>"},{"location":"api/#multicons.utils.linear_closed_itemsets_miner","title":"<code>linear_closed_itemsets_miner(membership_matrix)</code>","text":"<p>Returns a list of frequent closed itemsets using the LCM algorithm.</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/utils.py</code> <pre><code>def linear_closed_itemsets_miner(membership_matrix: pd.DataFrame):\n\"\"\"Returns a list of frequent closed itemsets using the LCM algorithm.\"\"\"\n\n    transactions = []\n    for i in membership_matrix.index:\n        transactions.append(np.nonzero(membership_matrix.iloc[i].values)[0].tolist())\n    frequent_closed_itemsets = eclat(transactions, target=\"c\", supp=0, algo=\"o\", conf=0)\n    return sorted(map(lambda x: frozenset(x[0]), frequent_closed_itemsets), key=len)\n</code></pre>"},{"location":"api/#multicons.consensus","title":"<code>multicons.consensus</code>","text":"<p>Consensus functions definitions.</p>"},{"location":"api/#multicons.consensus.consensus_function_10","title":"<code>consensus_function_10(bi_clust, merging_threshold=None)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_10(bi_clust: list[set], merging_threshold=None):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    i = 0\n    count = len(bi_clust)\n    while i &lt; count - 1:\n        bi_clust_i = bi_clust[i]\n        j = i + 1\n        while j &lt; count:\n            bi_clust_j = bi_clust[j]\n            intersection_size = len(bi_clust_i.intersection(bi_clust_j))\n            if intersection_size == 0:\n                j += 1\n                continue\n            if intersection_size == len(bi_clust_i):\n                # Bi\u2282Bj\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                break\n            if intersection_size == len(bi_clust_j):\n                # Bj\u2282Bi\n                del bi_clust[j]\n                count -= 1\n                continue\n            bi_clust[j] = bi_clust_i.union(bi_clust_j)\n            del bi_clust[i]\n            count -= 1\n            i -= 1\n            break\n        i += 1\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_12","title":"<code>consensus_function_12(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_12(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    i = 0\n    count = len(bi_clust)\n    while i &lt; count - 1:\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        j = i + 1\n        while j &lt; count:\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            bi_clust_intersection = bi_clust_i.intersection(bi_clust_j)\n            intersection_size = len(bi_clust_intersection)\n            if intersection_size == 0:\n                j += 1\n                continue\n            if intersection_size == bi_clust_i_size:\n                # Bi\u2282Bj\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                break\n            if intersection_size == bi_clust_j_size:\n                # Bj\u2282Bi\n                del bi_clust[j]\n                count -= 1\n                continue\n            if (\n                intersection_size &gt;= bi_clust_i_size * merging_threshold\n                or intersection_size &gt;= bi_clust_j_size * merging_threshold\n            ):\n                # Merge intersecting sets (Bj\u2229Bi / |Bi| &gt; MT or Bj\u2229Bi / |Bj| &gt; MT)\n                bi_clust[j] = bi_clust_i.union(bi_clust_j)\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                break\n            # Split intersecting sets (remove intersection from bigger set)\n            if bi_clust_i_size &lt;= bi_clust_j_size:\n                bi_clust[j] = bi_clust_j - bi_clust_intersection\n                continue\n            bi_clust[i] = bi_clust_i - bi_clust_intersection\n            i -= 1\n            break\n        i += 1\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_13","title":"<code>consensus_function_13(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_13(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    i = 0\n    count = len(bi_clust)\n    merging_threshold *= 2\n    while i &lt; count - 1:\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        j = i + 1\n        best_intersection_ratio = 0\n        best_intersection_ratio_j = 0\n        broken = False\n        while j &lt; count:\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            bi_clust_intersection = bi_clust_i.intersection(bi_clust_j)\n            intersection_size = len(bi_clust_intersection)\n            if intersection_size == 0:\n                j += 1\n                continue\n            if intersection_size == bi_clust_i_size:\n                # Bi\u2282Bj\n                del bi_clust[i]\n                count -= 1\n                i -= 1\n                broken = True\n                break\n            if intersection_size == bi_clust_j_size:\n                # Bj\u2282Bi\n                del bi_clust[j]\n                count -= 1\n                continue\n            average_intersection_ratio = (\n                intersection_size\n                * (bi_clust_j_size + bi_clust_i_size)\n                / (bi_clust_j_size * bi_clust_i_size)\n            )\n            if average_intersection_ratio &gt; best_intersection_ratio:\n                best_intersection_ratio = average_intersection_ratio\n                best_intersection_ratio_j = j\n            j += 1\n\n        if not broken and best_intersection_ratio &gt; 0:\n            if best_intersection_ratio &gt;= merging_threshold:\n                # Merge\n                bi_clust[best_intersection_ratio_j] = bi_clust_i.union(\n                    bi_clust[best_intersection_ratio_j]\n                )\n                del bi_clust[i]\n                count -= 1\n                continue\n            # Split\n            if bi_clust_i_size &lt;= bi_clust_j_size:\n                bi_clust[best_intersection_ratio_j] = (\n                    bi_clust[best_intersection_ratio_j] - bi_clust_i\n                )\n                continue\n            bi_clust[i] = bi_clust_i - bi_clust[best_intersection_ratio_j]\n            continue\n        i += 1\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_14","title":"<code>consensus_function_14(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_14(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    while True:\n        _remove_subsets(bi_clust)\n        bi_clust_size = len(bi_clust)\n        if bi_clust_size == 1:\n            return\n        intersection_matrix = pd.DataFrame(\n            columns=range(bi_clust_size), index=range(bi_clust_size), dtype=int\n        )\n\n        for i in range(bi_clust_size - 1):\n            bi_clust_i = bi_clust[i]\n            bi_clust_i_size = len(bi_clust_i)\n            for j in range(i + 1, bi_clust_size):\n                bi_clust_j = bi_clust[j]\n                bi_clust_j_size = len(bi_clust_j)\n                intersection_size = len(bi_clust_i.intersection(bi_clust_j))\n                if intersection_size == 0:\n                    continue\n                intersection_matrix.iloc[i, j] = intersection_size / bi_clust_i_size\n                intersection_matrix.iloc[j, i] = intersection_size / bi_clust_j_size\n\n        if intersection_matrix.isna().values.all():\n            break\n\n        pointer = pd.DataFrame(columns=range(3), index=range(bi_clust_size))\n        for i in range(bi_clust_size):\n            if not intersection_matrix.iloc[i, :].isna().all():\n                pointer.iloc[i, 0] = i\n                pointer.iloc[i, 1] = intersection_matrix.iloc[i, :].argmax()\n                pointer.iloc[i, 2] = intersection_matrix.iloc[i, pointer.iloc[i, 1]]\n\n        pointer.sort_values(2, inplace=True, ascending=False)\n        pointer = pointer[pointer.iloc[:, 2] &gt; 0.0]\n\n        for k in range(pointer.shape[0]):\n            i = pointer.iloc[k, 0]\n            j = pointer.iloc[k, 1]\n            value = intersection_matrix.iloc[i, j]\n            if value is None:\n                continue\n            if value &gt;= merging_threshold:\n                bi_clust[i] = bi_clust[i].union(bi_clust[j])\n                bi_clust[j] = set()\n                intersection_matrix.iloc[i, :] = None\n                intersection_matrix.iloc[:, j] = None\n                continue\n            if len(bi_clust[i]) &lt;= len(bi_clust[j]):\n                bi_clust[j] = bi_clust[j] - bi_clust[i]\n                intersection_matrix.iloc[j, :] = None\n                intersection_matrix.iloc[:, j] = None\n                continue\n            bi_clust[i] = bi_clust[i] - bi_clust[j]\n            intersection_matrix.iloc[i, :] = None\n            intersection_matrix.iloc[:, i] = None\n</code></pre>"},{"location":"api/#multicons.consensus.consensus_function_15","title":"<code>consensus_function_15(bi_clust, merging_threshold=0.5)</code>","text":"<p>Returns a modified bi_clust (set of unique instance sets).</p> Source code in <code>/home/circleci/.local/lib/python3.10/site-packages/multicons/consensus.py</code> <pre><code>def consensus_function_15(bi_clust: list[set], merging_threshold: float = 0.5):\n\"\"\"Returns a modified bi_clust (set of unique instance sets).\"\"\"\n\n    _remove_subsets(bi_clust)\n    bi_clust_size = len(bi_clust)\n    if bi_clust_size == 1:\n        return\n    intersection_matrix = pd.DataFrame(\n        0, columns=range(bi_clust_size), index=range(bi_clust_size), dtype=int\n    )\n    for i in range(bi_clust_size - 1):\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        for j in range(i + 1, bi_clust_size):\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            intersection_size = len(bi_clust_i.intersection(bi_clust_j))\n            if intersection_size == 0:\n                continue\n            intersection_matrix.iloc[i, j] = intersection_size / bi_clust_i_size\n            intersection_matrix.iloc[j, i] = intersection_size / bi_clust_j_size\n\n    cluster_indexes = sp.coo_matrix(intersection_matrix &gt;= merging_threshold).nonzero()\n    for index, i in enumerate(cluster_indexes[0]):\n        j = cluster_indexes[1][index]\n        bi_clust[i] = bi_clust[j] = bi_clust[i].union(bi_clust[j])\n\n    _remove_subsets(bi_clust)\n    bi_clust_size = len(bi_clust)\n    if bi_clust_size == 1:\n        return\n\n    for i in range(bi_clust_size - 1):\n        bi_clust_i = bi_clust[i]\n        bi_clust_i_size = len(bi_clust_i)\n        for j in range(i + 1, bi_clust_size):\n            bi_clust_j = bi_clust[j]\n            bi_clust_j_size = len(bi_clust_j)\n            bi_clust_intersection = bi_clust_i.intersection(bi_clust_j)\n            if len(bi_clust_intersection) == 0:\n                continue\n            if bi_clust_i_size &lt;= bi_clust_j_size:\n                bi_clust[j] = bi_clust_j - bi_clust_intersection\n                continue\n            bi_clust[i] = bi_clust_i - bi_clust_intersection\n\n    _remove_subsets(bi_clust)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"In\u00a0[1]: Copied! <pre>from os import path\n\nimport numpy as np\nimport pandas as pd\nfrom fcmeans import FCM\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import (\n    DBSCAN,\n    AgglomerativeClustering,\n    Birch,\n    KMeans,\n    SpectralClustering,\n)\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn_extra.cluster import KMedoids\n\nfrom multicons import MultiCons\nfrom multicons.utils import jaccard_similarity\n\nnp.set_printoptions(threshold=100)\n</pre> from os import path  import numpy as np import pandas as pd from fcmeans import FCM from matplotlib import pyplot as plt from sklearn.cluster import (     DBSCAN,     AgglomerativeClustering,     Birch,     KMeans,     SpectralClustering, ) from sklearn.mixture import GaussianMixture from sklearn_extra.cluster import KMedoids  from multicons import MultiCons from multicons.utils import jaccard_similarity  np.set_printoptions(threshold=100) In\u00a0[2]: Copied! <pre># Load the data\nfile_prefix = \"\" if path.exists(\"cassini.csv\") else \"docs/\"\nfile_name = f\"{file_prefix}cassini.csv\"\ncassini = pd.read_csv(file_name)\n# Remove the class labels\ncassini_train_data = cassini.drop(['class'], axis=1)\n</pre> # Load the data file_prefix = \"\" if path.exists(\"cassini.csv\") else \"docs/\" file_name = f\"{file_prefix}cassini.csv\" cassini = pd.read_csv(file_name) # Remove the class labels cassini_train_data = cassini.drop(['class'], axis=1) In\u00a0[3]: Copied! <pre># Setup the plot axes\nfig, axes = plt.subplots(\n    nrows=3, ncols=3, figsize=(18, 12), sharex=True, sharey=True\n)\n# Common plot arguments\ncommon_kwargs = {\"x\": \"x\", \"y\": \"y\", \"colorbar\": False, \"colormap\": \"Paired\"}\n# Our collection of base clusterings\nbase_clusterings = []\n\n# Cassini\ncassini.plot.scatter(c=\"class\", title=\"Cassini\", ax=axes[0, 0], **common_kwargs)\n\n# K-means\nbase_clusterings.append(KMeans(n_clusters=3).fit_predict(cassini_train_data))\ncassini_train_data.plot.scatter(\n    title=\"K-means\", ax=axes[0, 1], c=base_clusterings[-1], **common_kwargs\n)\n\n# Average linkage\nbase_clusterings.append(\n    AgglomerativeClustering(n_clusters=3).fit_predict(cassini_train_data)\n)\ncassini_train_data.plot.scatter(\n    title=\"Average linkage\", ax=axes[0, 2], c=base_clusterings[-1], **common_kwargs\n)\n\n# Gaussian model\nbase_clusterings.append(\n    GaussianMixture(n_components=3, random_state=5).fit_predict(cassini_train_data)\n)\ncassini_train_data.plot.scatter(\n    title=\"Gaussian model\", ax=axes[1, 0], c=base_clusterings[-1], **common_kwargs\n)\n\n# C-means\nfcm = FCM(n_clusters=3, max_iter=5, m=5)\nfcm.fit(cassini_train_data.values)\nbase_clusterings.append(fcm.predict(cassini_train_data.values))\ncassini_train_data.plot.scatter(\n    title=\"C-means\", ax=axes[1, 1], c=base_clusterings[-1], **common_kwargs\n)\n\n# PAM\nbase_clusterings.append(KMedoids(n_clusters=3).fit_predict(cassini_train_data))\ncassini_train_data.plot.scatter(\n    title=\"PAM\", ax=axes[1, 2], c=base_clusterings[-1], **common_kwargs\n)\n\n# BIRCH\nbirch = Birch(n_clusters=3, threshold=0.5)\nbase_clusterings.append(birch.fit_predict(np.ascontiguousarray(cassini_train_data)))\ncassini_train_data.plot.scatter(\n    title=\"BIRCH\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs\n)\n\n# Spectral\nbase_clusterings.append(\n    SpectralClustering(n_clusters=3).fit_predict(cassini_train_data)\n)\ncassini_train_data.plot.scatter(\n    title=\"Spectral\", ax=axes[2, 1], c=base_clusterings[-1], **common_kwargs\n)\n\n# DBSCAN\nbase_clusterings.append(DBSCAN(eps=0.2).fit_predict(cassini_train_data))\ncassini_train_data.plot.scatter(\n    title=\"DBSCAN\", ax=axes[2, 2], c=base_clusterings[-1], **common_kwargs\n)\n\nfig.show()\n</pre> # Setup the plot axes fig, axes = plt.subplots(     nrows=3, ncols=3, figsize=(18, 12), sharex=True, sharey=True ) # Common plot arguments common_kwargs = {\"x\": \"x\", \"y\": \"y\", \"colorbar\": False, \"colormap\": \"Paired\"} # Our collection of base clusterings base_clusterings = []  # Cassini cassini.plot.scatter(c=\"class\", title=\"Cassini\", ax=axes[0, 0], **common_kwargs)  # K-means base_clusterings.append(KMeans(n_clusters=3).fit_predict(cassini_train_data)) cassini_train_data.plot.scatter(     title=\"K-means\", ax=axes[0, 1], c=base_clusterings[-1], **common_kwargs )  # Average linkage base_clusterings.append(     AgglomerativeClustering(n_clusters=3).fit_predict(cassini_train_data) ) cassini_train_data.plot.scatter(     title=\"Average linkage\", ax=axes[0, 2], c=base_clusterings[-1], **common_kwargs )  # Gaussian model base_clusterings.append(     GaussianMixture(n_components=3, random_state=5).fit_predict(cassini_train_data) ) cassini_train_data.plot.scatter(     title=\"Gaussian model\", ax=axes[1, 0], c=base_clusterings[-1], **common_kwargs )  # C-means fcm = FCM(n_clusters=3, max_iter=5, m=5) fcm.fit(cassini_train_data.values) base_clusterings.append(fcm.predict(cassini_train_data.values)) cassini_train_data.plot.scatter(     title=\"C-means\", ax=axes[1, 1], c=base_clusterings[-1], **common_kwargs )  # PAM base_clusterings.append(KMedoids(n_clusters=3).fit_predict(cassini_train_data)) cassini_train_data.plot.scatter(     title=\"PAM\", ax=axes[1, 2], c=base_clusterings[-1], **common_kwargs )  # BIRCH birch = Birch(n_clusters=3, threshold=0.5) base_clusterings.append(birch.fit_predict(np.ascontiguousarray(cassini_train_data))) cassini_train_data.plot.scatter(     title=\"BIRCH\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs )  # Spectral base_clusterings.append(     SpectralClustering(n_clusters=3).fit_predict(cassini_train_data) ) cassini_train_data.plot.scatter(     title=\"Spectral\", ax=axes[2, 1], c=base_clusterings[-1], **common_kwargs )  # DBSCAN base_clusterings.append(DBSCAN(eps=0.2).fit_predict(cassini_train_data)) cassini_train_data.plot.scatter(     title=\"DBSCAN\", ax=axes[2, 2], c=base_clusterings[-1], **common_kwargs )  fig.show() <pre>/home/circleci/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> <p>At this point, <code>base_clusterings</code> now contains all clustering candidates in a list (of lists):</p> In\u00a0[4]: Copied! <pre>np.array(base_clusterings)\n</pre> np.array(base_clusterings) Out[4]: <pre>array([[1, 1, 1, ..., 1, 2, 1],\n       [1, 1, 1, ..., 2, 2, 2],\n       [2, 2, 2, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 2, 1],\n       [0, 0, 0, ..., 2, 2, 2]])</pre> <p>Note: This MultiCons implementation requires the clustering labels to be numerical!</p> <p>Now, let's compute the consensus candidates with MultiCons:</p> In\u00a0[5]: Copied! <pre># MultiCons implementation aims to follow scikit-learn conventions.\nconsensus = MultiCons().fit(base_clusterings)\nconsensus\n</pre> # MultiCons implementation aims to follow scikit-learn conventions. consensus = MultiCons().fit(base_clusterings) consensus Out[5]: <pre>MultiCons(consensus_function=&lt;function consensus_function_10 at 0x7fd38c064af0&gt;,\n          similarity_measure=&lt;function jaccard_similarity at 0x7fd38c0bc820&gt;)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultiCons<pre>MultiCons(consensus_function=&lt;function consensus_function_10 at 0x7fd38c064af0&gt;,\n          similarity_measure=&lt;function jaccard_similarity at 0x7fd38c0bc820&gt;)</pre> In\u00a0[6]: Copied! <pre># The `consensus_vectors` attribute is a python list containing the\n# consensus candidates.\n# We transform it to a numpy array to better visualize it here.\nnp.array(consensus.consensus_vectors)\n</pre> # The `consensus_vectors` attribute is a python list containing the # consensus candidates. # We transform it to a numpy array to better visualize it here. np.array(consensus.consensus_vectors) Out[6]: <pre>array([[ 0,  0,  0, ...,  0,  0,  0],\n       [ 1,  1,  1, ...,  1,  1,  1],\n       [ 2,  2,  2, ...,  0,  0,  0],\n       [22,  9, 22, ..., 19, 18,  7]])</pre> In\u00a0[7]: Copied! <pre># The `decision_thresholds` attribute contains a list of decision thresholds\n# for each consensus vector.\nconsensus.decision_thresholds\n</pre> # The `decision_thresholds` attribute contains a list of decision thresholds # for each consensus vector. consensus.decision_thresholds Out[7]: <pre>[4, 6, 7, 8]</pre> In\u00a0[8]: Copied! <pre># The `recommended` attribute contains the index of the recommended consensus\n# vector\nconsensus.recommended\n</pre> # The `recommended` attribute contains the index of the recommended consensus # vector consensus.recommended Out[8]: <pre>2</pre> In\u00a0[9]: Copied! <pre># The `labels_` attribute contains the recommended consensus vector\nconsensus.labels_\n</pre> # The `labels_` attribute contains the recommended consensus vector consensus.labels_ Out[9]: <pre>array([2, 2, 2, ..., 0, 0, 0])</pre> In\u00a0[10]: Copied! <pre># Plot the recommended consensus clustering solution\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\ncassini_train_data.plot.scatter(\n    title=\"MultiCons\", ax=axes, c=consensus.labels_, **common_kwargs\n)\nfig.show()\n</pre> # Plot the recommended consensus clustering solution fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4)) cassini_train_data.plot.scatter(     title=\"MultiCons\", ax=axes, c=consensus.labels_, **common_kwargs ) fig.show() In\u00a0[11]: Copied! <pre># The `stability` attribute contains a list of stability values\n# for each consensus vector.\nconsensus.stability\n</pre> # The `stability` attribute contains a list of stability values # for each consensus vector. consensus.stability Out[11]: <pre>[4, 2, 1, 1]</pre> In\u00a0[12]: Copied! <pre># The `tree_quality` member contains a measure of the tree quality.\n# The measure ranges between 0 and 1. Higher is better.\nconsensus.tree_quality\n</pre> # The `tree_quality` member contains a measure of the tree quality. # The measure ranges between 0 and 1. Higher is better. consensus.tree_quality Out[12]: <pre>0.625</pre> In\u00a0[13]: Copied! <pre># The `ensemble_similarity` contains a list of ensemble similarity measures\n# for each consensus vector.\n# They are between 0 and 1. Higher is better.\nconsensus.ensemble_similarity\n</pre> # The `ensemble_similarity` contains a list of ensemble similarity measures # for each consensus vector. # They are between 0 and 1. Higher is better. consensus.ensemble_similarity Out[13]: <pre>array([0.37681381, 0.68329248, 0.79831989, 0.59432678])</pre> <p>Finally, let's visualize the consenus candidates using the ConsTree method:</p> In\u00a0[14]: Copied! <pre>cons_tree = consensus.cons_tree()\ncons_tree\n</pre> cons_tree = consensus.cons_tree() cons_tree Out[14]: %3 ConsTree Tree Quality = 0.625 cluster Legend 00 1000 10 400 00-&gt;10 11 600 00-&gt;11 legend_0 DT=4 ST=4 Similarity=0.38 legend_1 DT=6 ST=2 Similarity=0.68 legend_0-&gt;legend_1 21 400 10-&gt;21 20 200 11-&gt;20 22 400 11-&gt;22 legend_2 DT=7 ST=1 Similarity=0.8 legend_1-&gt;legend_2 31 1 20-&gt;31 32 1 20-&gt;32 35 2 20-&gt;35 37 4 20-&gt;37 38 5 20-&gt;38 312 7 20-&gt;312 313 8 20-&gt;313 316 24 20-&gt;316 317 27 20-&gt;317 318 35 20-&gt;318 319 86 20-&gt;319 34 2 21-&gt;34 310 6 21-&gt;310 314 10 21-&gt;314 315 18 21-&gt;315 320 174 21-&gt;320 321 190 21-&gt;321 30 1 22-&gt;30 33 1 22-&gt;33 36 3 22-&gt;36 39 6 22-&gt;39 311 6 22-&gt;311 322 383 22-&gt;322 legend_3 DT=8 ST=1 Similarity=0.59 legend_2-&gt;legend_3 In\u00a0[15]: Copied! <pre># Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}CassiniConsTree.svg\", cleanup=True)\n</pre> # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}CassiniConsTree.svg\", cleanup=True) Out[15]: <pre>'docs/CassiniConsTree.svg'</pre> <p>View ConsTree graph in full size: CassiniConsTree.svg</p> In\u00a0[16]: Copied! <pre>cov = np.array([[0.3, 0.1], [0.1, 0.3]])\ngaussian_distributions = pd.DataFrame(\n    np.concatenate(\n        (\n            np.concatenate(\n                (\n                    np.random.multivariate_normal([-0.25, -2], cov, 400),\n                    np.random.multivariate_normal([-1.75, -0.5], cov, 400),\n                    np.random.multivariate_normal([-0.5, 2], cov, 400),\n                    np.random.multivariate_normal([1.75, 2], cov, 400),\n                    np.random.multivariate_normal([2, -0.5], cov, 400),\n                ),\n            ),\n            np.concatenate(\n                (\n                    np.repeat([[1]], 400, 0),\n                    np.repeat([[2]], 400, 0),\n                    np.repeat([[3]], 400, 0),\n                    np.repeat([[4]], 400, 0),\n                    np.repeat([[5]], 400, 0),\n                )\n            )\n        ),\n        axis=1\n    ),\n    columns=[\"x\", \"y\", \"class\"],\n)\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\ngaussian_distributions.plot.scatter(\n    title=\"5 Overlapping Gaussians\", ax=axes, c=\"class\", **common_kwargs\n)\nfig.show()\n</pre> cov = np.array([[0.3, 0.1], [0.1, 0.3]]) gaussian_distributions = pd.DataFrame(     np.concatenate(         (             np.concatenate(                 (                     np.random.multivariate_normal([-0.25, -2], cov, 400),                     np.random.multivariate_normal([-1.75, -0.5], cov, 400),                     np.random.multivariate_normal([-0.5, 2], cov, 400),                     np.random.multivariate_normal([1.75, 2], cov, 400),                     np.random.multivariate_normal([2, -0.5], cov, 400),                 ),             ),             np.concatenate(                 (                     np.repeat([[1]], 400, 0),                     np.repeat([[2]], 400, 0),                     np.repeat([[3]], 400, 0),                     np.repeat([[4]], 400, 0),                     np.repeat([[5]], 400, 0),                 )             )         ),         axis=1     ),     columns=[\"x\", \"y\", \"class\"], ) fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4)) gaussian_distributions.plot.scatter(     title=\"5 Overlapping Gaussians\", ax=axes, c=\"class\", **common_kwargs ) fig.show() In\u00a0[17]: Copied! <pre># Remove the class labels\ngaussian_train_data = gaussian_distributions.drop(['class'], axis=1)\n</pre> # Remove the class labels gaussian_train_data = gaussian_distributions.drop(['class'], axis=1) <p>Next, let's compute the base clusterings and visualize their outcome:</p> In\u00a0[18]: Copied! <pre># Setup the plot axes\nfig, axes = plt.subplots(\n    nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True\n)\n# Our collection of base clusterings\nbase_clusterings = []\n\n# K-means (4 clusters)\nbase_clusterings.append(KMeans(n_clusters=4).fit_predict(gaussian_train_data))\ngaussian_train_data.plot.scatter(\n    title=\"K-means (4 clusters)\",\n    ax=axes[0, 0],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# Average linkage (9 clusters)\nbase_clusterings.append(\n    AgglomerativeClustering(n_clusters=9, linkage=\"average\").fit_predict(\n        gaussian_train_data\n    )\n)\ngaussian_train_data.plot.scatter(\n    title=\"Average linkage (9 clusters)\",\n    ax=axes[0, 1],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# Gaussian model (8 clusters)\nbase_clusterings.append(\n    GaussianMixture(n_components=8, random_state=2, reg_covar=0.2).fit_predict(\n        gaussian_train_data\n    )\n)\ngaussian_train_data.plot.scatter(\n    title=\"Gaussian model (8 clusters)\",\n    ax=axes[1, 0],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# C-means (2 clusters)\nfcm = FCM(n_clusters=2, max_iter=5, m=5)\nfcm.fit(gaussian_train_data.values)\nbase_clusterings.append(fcm.predict(gaussian_train_data.values))\ngaussian_train_data.plot.scatter(\n    title=\"C-means (2 clusters)\",\n    ax=axes[1, 1],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\n# PAM (3 clusters)\nbase_clusterings.append(KMedoids(n_clusters=3).fit_predict(gaussian_train_data))\ngaussian_train_data.plot.scatter(\n    title=\"PAM (3 clusters)\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs\n)\n\n# BIRCH (5 clusters)\nbirch = Birch(n_clusters=6, threshold=0.5)\nbase_clusterings.append(birch.fit_predict(gaussian_train_data))\ngaussian_train_data.plot.scatter(\n    title=\"BIRCH (6 clusters)\",\n    ax=axes[2, 1],\n    c=base_clusterings[-1],\n    **common_kwargs\n)\n\nfig.show()\n</pre> # Setup the plot axes fig, axes = plt.subplots(     nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True ) # Our collection of base clusterings base_clusterings = []  # K-means (4 clusters) base_clusterings.append(KMeans(n_clusters=4).fit_predict(gaussian_train_data)) gaussian_train_data.plot.scatter(     title=\"K-means (4 clusters)\",     ax=axes[0, 0],     c=base_clusterings[-1],     **common_kwargs )  # Average linkage (9 clusters) base_clusterings.append(     AgglomerativeClustering(n_clusters=9, linkage=\"average\").fit_predict(         gaussian_train_data     ) ) gaussian_train_data.plot.scatter(     title=\"Average linkage (9 clusters)\",     ax=axes[0, 1],     c=base_clusterings[-1],     **common_kwargs )  # Gaussian model (8 clusters) base_clusterings.append(     GaussianMixture(n_components=8, random_state=2, reg_covar=0.2).fit_predict(         gaussian_train_data     ) ) gaussian_train_data.plot.scatter(     title=\"Gaussian model (8 clusters)\",     ax=axes[1, 0],     c=base_clusterings[-1],     **common_kwargs )  # C-means (2 clusters) fcm = FCM(n_clusters=2, max_iter=5, m=5) fcm.fit(gaussian_train_data.values) base_clusterings.append(fcm.predict(gaussian_train_data.values)) gaussian_train_data.plot.scatter(     title=\"C-means (2 clusters)\",     ax=axes[1, 1],     c=base_clusterings[-1],     **common_kwargs )  # PAM (3 clusters) base_clusterings.append(KMedoids(n_clusters=3).fit_predict(gaussian_train_data)) gaussian_train_data.plot.scatter(     title=\"PAM (3 clusters)\", ax=axes[2, 0], c=base_clusterings[-1], **common_kwargs )  # BIRCH (5 clusters) birch = Birch(n_clusters=6, threshold=0.5) base_clusterings.append(birch.fit_predict(gaussian_train_data)) gaussian_train_data.plot.scatter(     title=\"BIRCH (6 clusters)\",     ax=axes[2, 1],     c=base_clusterings[-1],     **common_kwargs )  fig.show() <pre>/home/circleci/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> <p>Now, let's compute the consensus candidates with MultiCons and visualize their outcome:</p> In\u00a0[19]: Copied! <pre>consensus_1 = MultiCons()\nconsensus_2 = MultiCons(consensus_function=\"consensus_function_12\")\nconsensus_3 = MultiCons(consensus_function=\"consensus_function_13\")\nconsensus_4 = MultiCons(consensus_function=\"consensus_function_14\")\nconsensus_5 = MultiCons(consensus_function=\"consensus_function_15\")\n\nconsensus_1.fit(base_clusterings)\nconsensus_2.fit(base_clusterings)\nconsensus_3.fit(base_clusterings)\nconsensus_4.fit(base_clusterings)\nconsensus_5.fit(base_clusterings)\n\n# Plot the recommended consensus clustering solutions\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 1\",\n    ax=axes[0, 0],\n    c=consensus_1.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 2\",\n    ax=axes[0, 1],\n    c=consensus_2.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 3\",\n    ax=axes[1, 0],\n    c=consensus_3.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 4\",\n    ax=axes[1, 1],\n    c=consensus_4.labels_,\n    **common_kwargs\n)\ngaussian_train_data.plot.scatter(\n    title=\"MultiCons Approach 5\",\n    ax=axes[2, 1],\n    c=consensus_5.labels_,\n    **common_kwargs\n)\nfig.show()\n</pre> consensus_1 = MultiCons() consensus_2 = MultiCons(consensus_function=\"consensus_function_12\") consensus_3 = MultiCons(consensus_function=\"consensus_function_13\") consensus_4 = MultiCons(consensus_function=\"consensus_function_14\") consensus_5 = MultiCons(consensus_function=\"consensus_function_15\")  consensus_1.fit(base_clusterings) consensus_2.fit(base_clusterings) consensus_3.fit(base_clusterings) consensus_4.fit(base_clusterings) consensus_5.fit(base_clusterings)  # Plot the recommended consensus clustering solutions fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12), sharex=True, sharey=True) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 1\",     ax=axes[0, 0],     c=consensus_1.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 2\",     ax=axes[0, 1],     c=consensus_2.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 3\",     ax=axes[1, 0],     c=consensus_3.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 4\",     ax=axes[1, 1],     c=consensus_4.labels_,     **common_kwargs ) gaussian_train_data.plot.scatter(     title=\"MultiCons Approach 5\",     ax=axes[2, 1],     c=consensus_5.labels_,     **common_kwargs ) fig.show() <p>Also, let's visualize the ConsTrees:</p> In\u00a0[20]: Copied! <pre>cons_tree = consensus_1.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree1.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_1.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree1.svg\", cleanup=True) cons_tree Out[20]: %3 ConsTree Tree Quality = 0.33333333333333337 cluster Legend 00 2000 10 1 00-&gt;10 11 1 00-&gt;11 12 1 00-&gt;12 13 1 00-&gt;13 14 1 00-&gt;14 15 1 00-&gt;15 16 1 00-&gt;16 17 2 00-&gt;17 18 2 00-&gt;18 19 2 00-&gt;19 110 2 00-&gt;110 111 2 00-&gt;111 112 2 00-&gt;112 113 2 00-&gt;113 114 2 00-&gt;114 115 3 00-&gt;115 116 3 00-&gt;116 117 4 00-&gt;117 118 4 00-&gt;118 119 7 00-&gt;119 120 9 00-&gt;120 121 10 00-&gt;121 122 11 00-&gt;122 123 18 00-&gt;123 124 27 00-&gt;124 125 27 00-&gt;125 126 29 00-&gt;126 127 29 00-&gt;127 128 35 00-&gt;128 129 56 00-&gt;129 130 56 00-&gt;130 131 70 00-&gt;131 132 140 00-&gt;132 133 155 00-&gt;133 134 274 00-&gt;134 135 293 00-&gt;135 136 321 00-&gt;136 137 396 00-&gt;137 legend_0 DT=5 ST=5 Similarity=0.28 legend_1 DT=6 ST=1 Similarity=0.49 legend_0-&gt;legend_1 <p>View ConsTree graph 1 in full size: GaussianConsTree1.svg</p> In\u00a0[21]: Copied! <pre>cons_tree = consensus_2.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree2.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_2.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree2.svg\", cleanup=True) cons_tree Out[21]: %3 ConsTree Tree Quality = 1 cluster Legend 00 906 11 779 00-&gt;11 12 844 00-&gt;12 01 1094 10 377 01-&gt;10 01-&gt;12 legend_0 DT=1 ST=1 Similarity=0.49 legend_1 DT=2 ST=1 Similarity=0.51 legend_0-&gt;legend_1 21 395 10-&gt;21 24 407 10-&gt;24 11-&gt;21 22 399 11-&gt;22 23 406 11-&gt;23 20 393 12-&gt;20 12-&gt;22 12-&gt;24 legend_2 DT=3 ST=1 Similarity=0.68 legend_1-&gt;legend_2 32 393 20-&gt;32 31 396 21-&gt;31 30 407 22-&gt;30 33 397 22-&gt;33 23-&gt;30 23-&gt;31 34 407 24-&gt;34 legend_3 DT=4 ST=1 Similarity=0.68 legend_2-&gt;legend_3 45 410 30-&gt;45 42 395 31-&gt;42 31-&gt;45 41 75 32-&gt;41 43 318 32-&gt;43 40 55 33-&gt;40 44 340 33-&gt;44 33-&gt;45 46 407 34-&gt;46 legend_4 DT=5 ST=1 Similarity=0.63 legend_3-&gt;legend_4 58 2 40-&gt;58 517 4 40-&gt;517 518 4 40-&gt;518 523 18 40-&gt;523 524 27 40-&gt;524 51 1 41-&gt;51 52 1 41-&gt;52 55 1 41-&gt;55 513 2 41-&gt;513 531 70 41-&gt;531 510 2 42-&gt;510 511 2 42-&gt;511 512 2 42-&gt;512 520 9 42-&gt;520 527 29 42-&gt;527 530 56 42-&gt;530 532 140 42-&gt;532 533 155 42-&gt;533 50 1 43-&gt;50 57 2 43-&gt;57 515 3 43-&gt;515 516 3 43-&gt;516 528 35 43-&gt;528 534 274 43-&gt;534 53 1 44-&gt;53 56 1 44-&gt;56 519 7 44-&gt;519 521 10 44-&gt;521 536 321 44-&gt;536 54 1 45-&gt;54 59 2 45-&gt;59 514 2 45-&gt;514 525 27 45-&gt;525 526 29 45-&gt;526 529 56 45-&gt;529 535 293 45-&gt;535 522 11 46-&gt;522 537 396 46-&gt;537 legend_5 DT=6 ST=1 Similarity=0.49 legend_4-&gt;legend_5 <p>View ConsTree graph 2 in full size: GaussianConsTree2.svg</p> In\u00a0[22]: Copied! <pre>cons_tree = consensus_3.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree3.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_3.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree3.svg\", cleanup=True) cons_tree Out[22]: %3 ConsTree Tree Quality = 1 cluster Legend 00 703 10 490 00-&gt;10 11 796 00-&gt;11 01 794 01-&gt;10 12 714 01-&gt;12 02 503 02-&gt;11 legend_0 DT=1 ST=1 Similarity=0.53 legend_1 DT=2 ST=1 Similarity=0.52 legend_0-&gt;legend_1 20 113 10-&gt;20 23 377 10-&gt;23 21 130 11-&gt;21 24 334 11-&gt;24 25 332 11-&gt;25 22 318 12-&gt;22 26 396 12-&gt;26 legend_2 DT=3 ST=1 Similarity=0.58 legend_1-&gt;legend_2 35 57 20-&gt;35 310 349 20-&gt;310 32 6 21-&gt;32 36 96 21-&gt;36 39 362 21-&gt;39 31 6 22-&gt;31 22-&gt;32 38 309 22-&gt;38 33 13 23-&gt;33 37 364 23-&gt;37 30 3 24-&gt;30 34 39 24-&gt;34 24-&gt;310 25-&gt;30 25-&gt;39 311 396 26-&gt;311 legend_3 DT=4 ST=1 Similarity=0.59 legend_2-&gt;legend_3 45 33 30-&gt;45 411 332 30-&gt;411 41 6 31-&gt;41 40 2 32-&gt;40 47 73 32-&gt;47 410 312 32-&gt;410 49 384 33-&gt;49 413 407 33-&gt;413 43 8 34-&gt;43 34-&gt;45 48 84 35-&gt;48 35-&gt;49 36-&gt;40 42 6 36-&gt;42 46 49 36-&gt;46 36-&gt;47 44 11 37-&gt;44 37-&gt;49 38-&gt;410 39-&gt;42 39-&gt;46 39-&gt;411 310-&gt;48 412 293 310-&gt;412 311-&gt;413 legend_4 DT=5 ST=1 Similarity=0.58 legend_3-&gt;legend_4 51 1 40-&gt;51 55 1 40-&gt;55 50 1 41-&gt;50 57 2 41-&gt;57 515 3 41-&gt;515 58 2 42-&gt;58 518 4 42-&gt;518 53 1 43-&gt;53 519 7 43-&gt;519 512 2 44-&gt;512 520 9 44-&gt;520 59 2 45-&gt;59 514 2 45-&gt;514 526 29 45-&gt;526 517 4 46-&gt;517 523 18 46-&gt;523 524 27 46-&gt;524 52 1 47-&gt;52 513 2 47-&gt;513 531 70 47-&gt;531 54 1 48-&gt;54 525 27 48-&gt;525 529 56 48-&gt;529 510 2 49-&gt;510 511 2 49-&gt;511 527 29 49-&gt;527 530 56 49-&gt;530 532 140 49-&gt;532 533 155 49-&gt;533 516 3 410-&gt;516 528 35 410-&gt;528 534 274 410-&gt;534 56 1 411-&gt;56 521 10 411-&gt;521 536 321 411-&gt;536 535 293 412-&gt;535 522 11 413-&gt;522 537 396 413-&gt;537 legend_5 DT=6 ST=1 Similarity=0.49 legend_4-&gt;legend_5 <p>View ConsTree graph 1 in full size: GaussianConsTree3.svg</p> In\u00a0[23]: Copied! <pre>cons_tree = consensus_4.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree4.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_4.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree4.svg\", cleanup=True) cons_tree Out[23]: %3 ConsTree Tree Quality = 1 cluster Legend 00 849 11 813 00-&gt;11 12 810 00-&gt;12 01 1151 10 377 01-&gt;10 01-&gt;11 01-&gt;12 legend_0 DT=1 ST=1 Similarity=0.47 legend_1 DT=2 ST=1 Similarity=0.53 legend_0-&gt;legend_1 20 434 10-&gt;20 22 393 11-&gt;22 23 387 11-&gt;23 24 396 11-&gt;24 12-&gt;20 21 390 12-&gt;21 12-&gt;23 legend_2 DT=3 ST=1 Similarity=0.65 legend_1-&gt;legend_2 31 408 20-&gt;31 32 412 20-&gt;32 34 395 20-&gt;34 21-&gt;31 33 373 21-&gt;33 30 412 22-&gt;30 22-&gt;32 23-&gt;30 23-&gt;33 24-&gt;32 legend_3 DT=4 ST=1 Similarity=0.67 legend_2-&gt;legend_3 40 79 30-&gt;40 41 318 30-&gt;41 42 391 30-&gt;42 43 409 31-&gt;43 44 396 31-&gt;44 32-&gt;41 45 407 32-&gt;45 33-&gt;42 33-&gt;43 34-&gt;44 legend_4 DT=5 ST=1 Similarity=0.65 legend_3-&gt;legend_4 51 1 40-&gt;51 52 1 40-&gt;52 55 1 40-&gt;55 513 2 40-&gt;513 517 4 40-&gt;517 531 70 40-&gt;531 50 1 41-&gt;50 57 2 41-&gt;57 515 3 41-&gt;515 516 3 41-&gt;516 528 35 41-&gt;528 534 274 41-&gt;534 53 1 42-&gt;53 56 1 42-&gt;56 58 2 42-&gt;58 518 4 42-&gt;518 519 7 42-&gt;519 521 10 42-&gt;521 523 18 42-&gt;523 524 27 42-&gt;524 536 321 42-&gt;536 59 2 43-&gt;59 514 2 43-&gt;514 525 27 43-&gt;525 526 29 43-&gt;526 529 56 43-&gt;529 535 293 43-&gt;535 54 1 44-&gt;54 510 2 44-&gt;510 511 2 44-&gt;511 512 2 44-&gt;512 520 9 44-&gt;520 527 29 44-&gt;527 530 56 44-&gt;530 532 140 44-&gt;532 533 155 44-&gt;533 522 11 45-&gt;522 537 396 45-&gt;537 legend_5 DT=6 ST=1 Similarity=0.49 legend_4-&gt;legend_5 <p>View ConsTree graph 4 in full size: GaussianConsTree4.svg</p> In\u00a0[24]: Copied! <pre>cons_tree = consensus_5.cons_tree()\n# Save the ConsTree graph to a file\ncons_tree.render(outfile=f\"{file_prefix}GaussianConsTree5.svg\", cleanup=True)\ncons_tree\n</pre> cons_tree = consensus_5.cons_tree() # Save the ConsTree graph to a file cons_tree.render(outfile=f\"{file_prefix}GaussianConsTree5.svg\", cleanup=True) cons_tree Out[24]: %3 ConsTree Tree Quality = 0.8333333333333334 cluster Legend 00 2000 10 770 00-&gt;10 11 780 00-&gt;11 12 450 00-&gt;12 legend_0 DT=2 ST=2 Similarity=0.28 legend_1 DT=3 ST=1 Similarity=0.47 legend_0-&gt;legend_1 20 393 10-&gt;20 21 419 10-&gt;21 11-&gt;21 22 774 11-&gt;22 12-&gt;22 23 414 12-&gt;23 legend_2 DT=4 ST=1 Similarity=0.54 legend_1-&gt;legend_2 31 396 20-&gt;31 21-&gt;31 33 391 21-&gt;33 34 409 21-&gt;34 30 82 22-&gt;30 32 315 22-&gt;32 22-&gt;33 23-&gt;31 23-&gt;32 35 407 23-&gt;35 legend_3 DT=5 ST=1 Similarity=0.65 legend_2-&gt;legend_3 41 1 30-&gt;41 42 1 30-&gt;42 45 1 30-&gt;45 413 2 30-&gt;413 416 3 30-&gt;416 417 4 30-&gt;417 431 70 30-&gt;431 44 1 31-&gt;44 410 2 31-&gt;410 411 2 31-&gt;411 412 2 31-&gt;412 420 9 31-&gt;420 427 29 31-&gt;427 430 56 31-&gt;430 432 140 31-&gt;432 433 155 31-&gt;433 40 1 32-&gt;40 47 2 32-&gt;47 415 3 32-&gt;415 428 35 32-&gt;428 434 274 32-&gt;434 43 1 33-&gt;43 46 1 33-&gt;46 48 2 33-&gt;48 418 4 33-&gt;418 419 7 33-&gt;419 421 10 33-&gt;421 423 18 33-&gt;423 424 27 33-&gt;424 436 321 33-&gt;436 49 2 34-&gt;49 414 2 34-&gt;414 425 27 34-&gt;425 426 29 34-&gt;426 429 56 34-&gt;429 435 293 34-&gt;435 422 11 35-&gt;422 437 396 35-&gt;437 legend_4 DT=6 ST=1 Similarity=0.49 legend_3-&gt;legend_4 <p>View ConsTree graph 5 in full size: GaussianConsTree5.svg</p> <p>Finally, let's compare the clustering results:</p> In\u00a0[25]: Copied! <pre>true_labels = gaussian_distributions[\"class\"].to_numpy()\n# We compare the results using the pair-wise Jaccard Similarity Measure\npd.DataFrame(\n    [\n        [\"K-means\", jaccard_similarity(base_clusterings[0], true_labels)],\n        [\"Average linkage\", jaccard_similarity(base_clusterings[1], true_labels)],\n        [\"Gaussian model\", jaccard_similarity(base_clusterings[2], true_labels)],\n        [\"C-means\", jaccard_similarity(base_clusterings[3], true_labels)],\n        [\"PAM\", jaccard_similarity(base_clusterings[4], true_labels)],\n        [\"BIRCH\", jaccard_similarity(base_clusterings[5], true_labels)],\n        [\"MultiCons_1\", jaccard_similarity(consensus_1.labels_, true_labels)],\n        [\"MultiCons_2\", jaccard_similarity(consensus_2.labels_, true_labels)],\n        [\"MultiCons_3\", jaccard_similarity(consensus_3.labels_, true_labels)],\n        [\"MultiCons_4\", jaccard_similarity(consensus_4.labels_, true_labels)],\n        [\"MultiCons_5\", jaccard_similarity(consensus_5.labels_, true_labels)],\n    ],\n    columns=[\"Algorithm\", \"Jaccard\"]\n)\n</pre> true_labels = gaussian_distributions[\"class\"].to_numpy() # We compare the results using the pair-wise Jaccard Similarity Measure pd.DataFrame(     [         [\"K-means\", jaccard_similarity(base_clusterings[0], true_labels)],         [\"Average linkage\", jaccard_similarity(base_clusterings[1], true_labels)],         [\"Gaussian model\", jaccard_similarity(base_clusterings[2], true_labels)],         [\"C-means\", jaccard_similarity(base_clusterings[3], true_labels)],         [\"PAM\", jaccard_similarity(base_clusterings[4], true_labels)],         [\"BIRCH\", jaccard_similarity(base_clusterings[5], true_labels)],         [\"MultiCons_1\", jaccard_similarity(consensus_1.labels_, true_labels)],         [\"MultiCons_2\", jaccard_similarity(consensus_2.labels_, true_labels)],         [\"MultiCons_3\", jaccard_similarity(consensus_3.labels_, true_labels)],         [\"MultiCons_4\", jaccard_similarity(consensus_4.labels_, true_labels)],         [\"MultiCons_5\", jaccard_similarity(consensus_5.labels_, true_labels)],     ],     columns=[\"Algorithm\", \"Jaccard\"] ) Out[25]: Algorithm Jaccard 0 K-means 0.650268 1 Average linkage 0.869315 2 Gaussian model 0.897647 3 C-means 0.346205 4 PAM 0.457109 5 BIRCH 0.632349 6 MultiCons_1 0.574177 7 MultiCons_2 0.906355 8 MultiCons_3 0.767299 9 MultiCons_4 0.863010 10 MultiCons_5 0.850869"},{"location":"examples/#examples","title":"Examples\u00b6","text":"<p>In this section we present some usage examples for MultiCons. We replicate the examples presented in the Thesis of Atheer Al-Najdi (A closed patterns-based approach to the consensus clustering problem).</p>"},{"location":"examples/#cassini-dataset","title":"Cassini dataset\u00b6","text":"<p>The dataset consists of 1000 instances, each represents a point in a 2D space, forming a structure of three clusters.</p> <p>As in the Thesis of Atheer Al-Najdi, we will try to cluster the dataset with 8 different clustering algorithms and then compute and visualize the consensus clustering candidates using the MultiCons and ConsTree methods.</p> <p>Let's get a first visual of the dataset and our base clusterings:</p>"},{"location":"examples/#5-overlapping-gaussian-distributions","title":"5 Overlapping Gaussian distributions\u00b6","text":"<p>Replicating the example with a synthetic dataset used in the thesis of Atheer that consist of:</p> <ul> <li>generating 5 overlapping Gaussian distributed points in a 2D features space</li> <li>appying 6 different clustering algorithms with random choices for K values   (in the range [2, 9])</li> <li>comparing the results of 5 different MultiCons consensus solutions (by   altering the consensus functions)</li> </ul> <p>Let's start by generating the dataset:</p>"}]}